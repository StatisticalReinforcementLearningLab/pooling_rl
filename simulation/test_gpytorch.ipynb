{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "#sys.path\n",
    "#sys.path.append('../../../home00/stomkins/pooling_rl/models')\n",
    "#sys.path.append('../../../home00/stomkins/pooling_rl/simulation')\n",
    "import pandas as pd\n",
    "sys.path\n",
    "sys.path.append('../models')\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import run_gpytorchkernel\n",
    "import operator\n",
    "import study\n",
    "import time as time_module\n",
    "\n",
    "import TS_personal_params_pooled as pp\n",
    "import TS_global_params_pooled as gtp\n",
    "from numpy.random import uniform\n",
    "import run_gpy\n",
    "#sys.path.append('../simulation')\n",
    "import TS_fancy_pooled\n",
    "import TS\n",
    "#import TS_fancy_pooled\n",
    "import eta\n",
    "import pooling_bandits as pb\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import feature_transformations as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_policy_params_TS(experiment,update_period,\\\n",
    "                                standardize=False,baseline_features=None,psi_features=None,\\\n",
    "                                responsivity_keys=None,algo_type=None):\n",
    "    #,'location_1','location_2','location_3'\n",
    "    #'continuous_temp',\n",
    "    global_p =gtp.TS_global_params(21,baseline_features=baseline_features,psi_features=psi_features, responsivity_keys= responsivity_keys)\n",
    "    personal_p = pp.TS_personal_params()\n",
    "    #global_p =gtp.TS_global_params(10,context_dimension)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #global_p.mu_dimension = 64\n",
    "    \n",
    "    global_p.kdim =24\n",
    "    #194\n",
    "    global_p.baseline_indices = [i for i in range(3+ 3*len(baseline_features))]\n",
    "    #[i for i in range(192)]\n",
    "    #[0,1,2,3,4,5,6]\n",
    "    global_p.psi_indices = [0] + [1+baseline_features.index(j) for j in psi_features] \\\n",
    "    + [len(baseline_features)+1] + [(2+len(baseline_features))+baseline_features.index(j) for j in psi_features]\n",
    "    #[0,64]\n",
    "    global_p.user_id_index =0\n",
    "    \n",
    "    global_p.psi_features =psi_features\n",
    "    #[0,64]\n",
    "    \n",
    "    #print(global_p.psi_indices )\n",
    "    \n",
    "    global_p.update_period = update_period\n",
    "    \n",
    "    global_p.standardize = standardize\n",
    "    \n",
    "    \n",
    "    \n",
    "    initial_context = [0 for i in range(global_p.theta_dim)]\n",
    "    \n",
    "    global_p.mus0= global_p.get_mu0(initial_context)\n",
    "    #global_p.get_mu0(initial_context)\n",
    "    global_p.mus1= global_p.get_mu1(global_p.num_baseline_features)\n",
    "    global_p.mus2= global_p.get_mu2(global_p.num_responsivity_features)\n",
    "    #np.array([.120,3.3,-.11])\n",
    "    #global_p.get_mu2(global_p.num_responsivity_features)\n",
    "    \n",
    "    #global_p.sigmas0= global_p.get_asigma(len( personal_p.mus0[person]))\n",
    "    global_p.sigmas1= global_p.get_asigma(global_p.num_baseline_features+1)\n",
    "    global_p.sigmas2= global_p.get_asigma( global_p.num_responsivity_features+1)\n",
    "    \n",
    "    #4.83\n",
    "    global_p.mu2_knot = np.array([0]+[0 for i in range(global_p.num_responsivity_features)])\n",
    "    global_p.mu1_knot = np.zeros(global_p.num_baseline_features+1)\n",
    "    global_p.sigma1_knot = np.eye(global_p.num_baseline_features+1)\n",
    "    global_p.sigma2_knot = np.eye(global_p.num_responsivity_features+1)\n",
    "    #print(type(personal_p))\n",
    "    \n",
    "    for person in experiment.population.keys():\n",
    "        #experiment.population[person].root = '../../regal/murphy_lab/pooling/distributions/'\n",
    "        initial_context = [0 for i in range(global_p.theta_dim)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if algo_type!='batch':\n",
    "            personal_p.mus0[person]= global_p.get_mu0(initial_context)\n",
    "            personal_p.mus1[person]= global_p.get_mu1(global_p.num_baseline_features)\n",
    "            personal_p.mus2[person]= global_p.get_mu2(global_p.num_responsivity_features)\n",
    "            \n",
    "            personal_p.sigmas0[person]= global_p.get_asigma(len( personal_p.mus0[person]))\n",
    "            personal_p.sigmas1[person]= global_p.get_asigma(global_p.num_baseline_features+1)\n",
    "            personal_p.sigmas2[person]= global_p.get_asigma( global_p.num_responsivity_features+1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        personal_p.batch[person]=[[] for i in range(len(experiment.person_to_time[person]))]\n",
    "        personal_p.batch_index[person]=0\n",
    "        \n",
    "        #personal_p.etas[person]=eta.eta()\n",
    "        \n",
    "        personal_p.last_update[person]=experiment.person_to_time[person][0]\n",
    "\n",
    "\n",
    "    return global_p ,personal_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_reward(beta,states,Z):\n",
    "    if Z is None:\n",
    "        \n",
    "        return np.dot(beta,states)\n",
    "    return np.dot(beta,states)+Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size=32\n",
    "experiment = study.study('../../Downloads/distributions/',pop_size,'_short_unstaggered_6','case_three',sim_number=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = ['tod','dow','pretreatment','location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob,person = initialize_policy_params_TS(experiment,7,standardize=False,baseline_features=baseline,psi_features=[],responsivity_keys=baseline,algo_type = None)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.mu_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_save(exp):\n",
    "    to_save  = {}\n",
    "    for pid,pdata in exp.population.items():\n",
    "        for time,context in pdata.history.items():\n",
    "            \n",
    "            key = '{}-{}-{}'.format(pid,time,pdata.gid)\n",
    "            to_save[key]=context\n",
    "    return to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_kind_of_simulation(experiment,policy=None,personal_policy_params=None,global_policy_params=None,generative_functions=None,which_gen=None,feat_trans = None,algo_type = None,case=None,sim_num=None):\n",
    "    #write_directory = '../../murphy_lab/lab/pooling/temp'\n",
    "    experiment.last_update_day=experiment.study_days[0]\n",
    "    tod_check = set([])\n",
    "    \n",
    "    \n",
    "    additives = []\n",
    "    \n",
    "    for time in experiment.study_days:\n",
    "\n",
    "        if time==experiment.last_update_day+pd.DateOffset(days=global_policy_params.update_period):\n",
    "            experiment.last_update_day=time\n",
    "\n",
    "            if global_policy_params.decision_times>2:\n",
    "                global_policy_params.last_global_update_time=time\n",
    "\n",
    "\n",
    "                if algo_type=='batch' or algo_type=='pooling':\n",
    "\n",
    "                    temp_hist = feat_trans.get_history_decision_time_avail(experiment,time)\n",
    "                    temp_hist= feat_trans.history_semi_continuous(temp_hist,global_policy_params)\n",
    "                    context,steps,probs,actions= feat_trans.get_form_TS(temp_hist)\n",
    "                    #print(steps)\n",
    "                    temp_data = feat_trans.get_phi_from_history_lookups(temp_hist)\n",
    "\n",
    "                    steps = feat_trans.get_RT_o(steps,temp_data[0],global_policy_params.mu_theta,global_policy_params.theta_dim)\n",
    "                    #print(steps)\n",
    "                    print('steps {}'.format(steps.std()))\n",
    "                    temp = TS.policy_update_ts_new( context,steps,probs,actions,global_policy_params.noise_term,\\\n",
    "                                                       global_policy_params.mu1_knot,\\\n",
    "                                                       global_policy_params.sigma1_knot,\\\n",
    "                                                       global_policy_params.mu2_knot,\\\n",
    "                                                       global_policy_params.sigma2_knot)\n",
    "\n",
    "                    mu_beta = temp[0]\n",
    "                    Sigma_beta = temp[1]\n",
    "                    #print(mu_beta)\n",
    "                    if algo_type=='batch':\n",
    "                        global_policy_params.update_mus(None,mu_beta,2)\n",
    "                        global_policy_params.update_sigmas(None,Sigma_beta,2)\n",
    "                    else :\n",
    "                        global_posterior = mu_beta\n",
    "                        global_posterior_sigma = Sigma_beta\n",
    "                        try:\n",
    "                            \n",
    "                            #print(baseline_features)\n",
    "                            temp_params = run_gpytorchkernel.run(temp_data[0], temp_data[1],steps,global_policy_params)\n",
    "                            print(temp_data[0].shape)\n",
    "                            print(temp_params)\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            print('was error')\n",
    "                           # print('global_info',e, time,global_policy_params.decision_times,'error in running gp',file=open('pooling/{}/updates_global_newbigtest_{}_{}_{}six_weeks_only_onoise_errorscurrent.txt'.format(case,len(experiment.population),global_policy_params.update_period,sim_num), 'a'))\n",
    "                        temp_params={'cov':global_policy_params.cov,'noise':global_policy_params.noise_term,'like':-100333,'sigma_u':global_policy_params.sigma_u}\n",
    "                        inv_term = pb.get_inv_term(temp_params['cov'],temp_data[0].shape[0],temp_params['noise'])\n",
    "                        global_policy_params.update_params(temp_params)\n",
    "                        global_policy_params.inv_term=inv_term\n",
    "                                    #print(temp_params)\n",
    "                        global_policy_params.history = temp_data\n",
    "                        \n",
    "\n",
    "\n",
    "        tod = feat_trans.get_time_of_day(time)\n",
    "        dow = feat_trans.get_day_of_week(time)\n",
    "\n",
    "        if time==experiment.study_days[0]:\n",
    "                        #print('init weather')\n",
    "            weather = feat_trans.get_weather_prior(tod,time.month,seed=experiment.weather_gen)\n",
    "                            #temperature = tf.continuous_temperature(weather)\n",
    "        elif time.hour in experiment.weather_update_hours and time.minute==0:\n",
    "            weather = feat_trans.get_next_weather(str(tod),str(time.month),weather,seed=experiment.weather_gen)\n",
    "\n",
    "        for person in experiment.dates_to_people[time]:\n",
    "            participant = experiment.population[person]\n",
    "            dt=int(time in participant.decision_times)\n",
    "            action = 0\n",
    "            prob=0\n",
    "\n",
    "\n",
    "            if algo_type=='personalized' and dt and time==participant.last_update_day+pd.DateOffset(days=global_policy_params.update_period):\n",
    "                temp_hist = feat_trans.get_history_decision_time_avail_single({participant.pid:participant.history},time)\n",
    "                temp_hist= feat_trans.history_semi_continuous(temp_hist,global_policy_params)\n",
    "                context,steps,probs,actions= feat_trans.get_form_TS(temp_hist)\n",
    "              \n",
    "                temp_data = feat_trans.get_phi_from_history_lookups(temp_hist)\n",
    "                steps = feat_trans.get_RT_o(steps,temp_data[0],global_policy_params.mu_theta,global_policy_params.theta_dim)\n",
    "               \n",
    "                temp = TS.policy_update_ts_new( context,steps,probs,actions,global_policy_params.noise_term,\\\n",
    "                                                           global_policy_params.mu1_knot,\\\n",
    "                                                           global_policy_params.sigma1_knot,\\\n",
    "                                                           global_policy_params.mu2_knot,\\\n",
    "                                                           global_policy_params.sigma2_knot,\n",
    "                                                           #personal_policy_params.mus1[participant.pid],\\\n",
    "                                                           #personal_policy_params.sigmas1[participant.pid],\\\n",
    "                                                           #personal_policy_params.mus2[participant.pid],\\\n",
    "                                                           #personal_policy_params.sigmas2[participant.pid],\n",
    "                                                           \n",
    "                                                           )\n",
    "                mu_beta = temp[0]\n",
    "                Sigma_beta = temp[1]\n",
    "                personal_policy_params.update_mus(participant.pid,mu_beta,2)\n",
    "                personal_policy_params.update_sigmas(participant.pid,Sigma_beta,2)\n",
    "                participant.last_update_day=time\n",
    "            elif algo_type=='pooling' and dt and global_policy_params.decision_times>2 and global_policy_params.history!=None and  time==participant.last_update_day+pd.DateOffset(days=global_policy_params.update_period):\n",
    "                history = global_policy_params.history\n",
    "                temp = simple_bandits.calculate_posterior_faster(global_policy_params,\\\n",
    "                                                         participant.pid,participant.current_day_counter,\\\n",
    "                                                         history[0], history[1],history[2] )\n",
    "            \n",
    "                                                         #global_posterior = mu_beta\n",
    "                                                         #global_posterior_sigma = Sigma_beta\n",
    "                personal_policy_params.update_mus(participant.pid,mu_beta,2)\n",
    "                personal_policy_params.update_sigmas(participant.pid,Sigma_beta,2)\n",
    "                participant.last_update_day=time\n",
    "            participant.set_tod(tod)\n",
    "            participant.set_dow(dow)\n",
    "\n",
    "            availability = (participant.rando_gen.uniform() < 0.8)\n",
    "    \n",
    "            participant.set_available(availability)\n",
    "            if time == participant.times[0]:\n",
    "                location = feat_trans.get_location_prior(str(participant.gid),str(tod),str(dow),seed = participant.rando_gen)\n",
    "            elif time.hour in experiment.location_update_hours and time.minute==0 :\n",
    "                location = feat_trans.get_next_location(participant.gid,tod,dow,participant.get_loc(),seed =participant.rando_gen)\n",
    "\n",
    "            participant.set_loc(location)\n",
    "\n",
    "            if time <= participant.times[0]:\n",
    "                        steps_last_time_period = 0\n",
    "            else:\n",
    "                if time.hour==0 and time.minute==0:\n",
    "                    participant.current_day_counter=participant.current_day_counter+1\n",
    "\n",
    "                steps_last_time_period = participant.steps\n",
    "\n",
    "            prob = -1\n",
    "            add=None\n",
    "            optimal_action = -1\n",
    "            optimal_reward = -100\n",
    "            if dt:\n",
    "                if policy=='TS':\n",
    "                    pretreatment = feat_trans.get_pretreatment(steps_last_time_period)\n",
    "                    z = [1]\n",
    "\n",
    "                    if 'tod' in global_policy_params.baseline_features:\n",
    "                        z.append(tod)\n",
    "                    if 'dow' in global_policy_params.baseline_features:\n",
    "                        z.append(dow)\n",
    "                    if 'pretreatment' in global_policy_params.baseline_features:\n",
    "                        z.append(pretreatment)\n",
    "                    if 'location' in global_policy_params.baseline_features:\n",
    "                        z.append(location)\n",
    "\n",
    "                    if algo_type=='batch':\n",
    "                        prob = TS.prob_cal_ts(z,0,global_policy_params.mus2,global_policy_params.sigmas2,global_policy_params,seed = experiment.algo_rando_gen)\n",
    "                    elif algo_type=='personalized':\n",
    "                        prob = TS.prob_cal_ts(z,0,personal_policy_params.mus2[participant.pid],personal_policy_params.sigmas2[participant.pid],global_policy_params,seed=experiment.algo_rando_gen)\n",
    "                    action = int(experiment.algo_rando_gen.uniform() < prob)\n",
    "                    if availability:\n",
    "                        context = [action,participant.gid,tod,dow,weather,pretreatment,location,\\\n",
    "                                       0,0,0]\n",
    "                        steps = feat_trans.get_steps_action(context,seed = participant.rando_gen)\n",
    "                        add = action*(feat_trans.get_add_no_action(z,experiment.beta,participant.Z))\n",
    "                        participant.steps = steps+add\n",
    "                        optimal_reward = get_optimal_reward(experiment.beta,z,participant.Z)\n",
    "                        optimal_action = int(optimal_reward>=0)\n",
    "                    else:\n",
    "                        steps = feat_trans.get_steps_no_action(participant.gid,tod,dow,location,\\\n",
    "                        pretreatment,weather,seed = participant.rando_gen)\n",
    "                        participant.steps = steps\n",
    "\n",
    "                    global_policy_params.decision_times =   global_policy_params.decision_times+1\n",
    "                else:\n",
    "                        steps = feat_trans.get_steps_no_action(participant.gid,tod,dow,location,\\\n",
    "                                                               pretreatment,weather,seed = participant.rando_gen)\n",
    "                        participant.steps = steps\n",
    "                context_dict =  {'steps':participant.steps,'add':add,'action':action,'location':location,'location_1':int(location==1),\\\n",
    "'ltps':steps_last_time_period,'location_2':int(location==2),'location_3':int(location==3),\\\n",
    "    'study_day':participant.current_day_counter,\\\n",
    "        'decision_time':dt,\\\n",
    "            'time':time,'avail':availability,'prob':prob,\\\n",
    "                'dow':dow,'tod':tod,'weather':weather,\\\n",
    "                    'pretreatment':feat_trans.get_pretreatment(steps_last_time_period),\\\n",
    "                        'optimal_reward':optimal_reward,'optimal_action':optimal_action,\\\n",
    "                            'mu2':global_policy_params.mus2,'gid':participant.gid}\n",
    "\n",
    "                participant.history[time]=context_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regret(experiment):\n",
    "    optimal_actions ={}\n",
    "    rewards = {}\n",
    "    actions = {}\n",
    "    for pid,person in experiment.population.items():\n",
    "        for time,data in person.history.items():\n",
    "            if data['decision_time'] and data['avail']:\n",
    "                key = time\n",
    "                if key not in optimal_actions:\n",
    "                    optimal_actions[key]=[]\n",
    "                if key not in rewards:\n",
    "                    rewards[key]=[]\n",
    "                if key not in actions:\n",
    "                    actions[key]=[]\n",
    "                if data['optimal_action']!=-1:\n",
    "                    optimal_actions[key].append(int(data['action']==data['optimal_action']))\n",
    "                    regret = int(data['action']!=data['optimal_action'])*(abs(data['optimal_reward']))\n",
    "                    rewards[key].append(regret)\n",
    "                    actions[key].append(data['action'])\n",
    "    return optimal_actions,rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_groupids(exp):\n",
    "    to_save  = {}\n",
    "    for pid,pdata in exp.population.items():\n",
    "        gid  = pdata.gid\n",
    "        key = 'participant-{}'.format(pid)\n",
    "        to_save[key]=gid\n",
    "    return to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_many(algo_type,cases,sim_start,sim_end,update_time,dist_root,write_directory):\n",
    "    for case in cases:\n",
    "        #,'case_two','case_three'\n",
    "        #case = 'case_one'\n",
    "        \n",
    "        \n",
    "        baseline = ['tod','dow','pretreatment','location']\n",
    "        \n",
    "        \n",
    "        \n",
    "        for u in [update_time]:\n",
    "            \n",
    "            all_actions = {}\n",
    "            all_rewards = {}\n",
    "            #'../../Downloads/distributions/'\n",
    "            feat_trans = ft.feature_transformation(dist_root)\n",
    "            \n",
    "            for sim in range(sim_start,sim_end):\n",
    "                pop_size=32\n",
    "                experiment = study.study(dist_root,pop_size,'_short_unstaggered_6',which_gen=case,sim_number=sim)\n",
    "                experiment.update_beta(set(baseline))\n",
    "                #print('beta')\n",
    "                #print(experiment.beta)\n",
    "                glob,personal = initialize_policy_params_TS(experiment,7,standardize=False,baseline_features=baseline,psi_features=[],responsivity_keys=baseline,algo_type =algo_type)\n",
    "                print(glob.rho_term)\n",
    "                hist = new_kind_of_simulation(experiment,'TS',personal,glob,feat_trans=feat_trans,algo_type=algo_type,case=case,sim_num=sim)\n",
    "                to_save = make_to_save(experiment)\n",
    "                actions,rewards = get_regret(experiment)\n",
    "                gids = make_to_groupids(experiment)\n",
    "                    #for i,a in actions.items():\n",
    "                    #if i not in all_actions:\n",
    "                    #all_actions[i]=a\n",
    "                    #else:\n",
    "                    #all_actions[i].extend(a)\n",
    "                    #for i,a in rewards.items():\n",
    "                    # if i not in all_rewards:\n",
    "                    #all_rewards[i]=a\n",
    "                    #else:\n",
    "                    #all_rewards[i].extend(a)\n",
    "            \n",
    "                #return experiment,personal\n",
    "                filename = '{}{}/population_size_{}_update_days_{}_{}_static_sim_{}_alltests.pkl'.format('{}{}/'.format(write_directory,algo_type),case,pop_size,u,'short',sim)\n",
    "                with open(filename,'wb') as f:\n",
    "                    pickle.dump({'gids':gids,'regrets':rewards,'actions':actions,'history':to_save},f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7393274299268684\n",
      "steps 1.3318587577121956\n",
      "torch.Size([896, 15])\n",
      "0\n",
      "5\n",
      "Iter 1/50 - Loss: 1.652\n",
      "Iter 2/50 - Loss: 1.638\n",
      "Iter 3/50 - Loss: 1.634\n",
      "Iter 4/50 - Loss: 1.629\n",
      "not enough values to unpack (expected 3, got 2)\n",
      "here\n",
      "(896, 15)\n",
      "{'sigma_u': array([[0.10992582, 0.16446694],\n",
      "       [0.16446694, 0.19577564]]), 'cov': array([[ 5.9767675,  5.9767675,  5.9767675, ...,  3.       ,  3.       ,\n",
      "         3.       ],\n",
      "       [ 5.9767675,  8.976768 ,  8.976768 , ...,  3.       ,  3.       ,\n",
      "         6.       ],\n",
      "       [ 5.9767675,  8.976768 , 11.976768 , ...,  3.       ,  6.       ,\n",
      "         9.       ],\n",
      "       ...,\n",
      "       [ 3.       ,  3.       ,  3.       , ...,  5.9767675,  5.9767675,\n",
      "         5.9767675],\n",
      "       [ 3.       ,  3.       ,  6.       , ...,  5.9767675,  8.976768 ,\n",
      "         8.976768 ],\n",
      "       [ 3.       ,  6.       ,  9.       , ...,  5.9767675,  8.976768 ,\n",
      "        11.976768 ]], dtype=float32), 'noise': tensor([[1.4252]], grad_fn=<AddBackward0>), 'like': 0}\n",
      "steps 1.3516317103809745\n",
      "torch.Size([1804, 15])\n",
      "0\n",
      "5\n",
      "Iter 1/50 - Loss: 1.671\n",
      "Iter 2/50 - Loss: 1.657\n",
      "Iter 3/50 - Loss: 1.650\n",
      "Iter 4/50 - Loss: 1.644\n",
      "Iter 5/50 - Loss: 1.623\n",
      "not enough values to unpack (expected 3, got 2)\n",
      "here\n",
      "(1804, 15)\n",
      "{'sigma_u': array([[0.1706693 , 0.18888113],\n",
      "       [0.18888113, 0.14334327]]), 'cov': array([[ 5.93625 ,  5.93625 ,  5.93625 , ...,  3.      ,  3.      ,\n",
      "         3.      ],\n",
      "       [ 5.93625 ,  8.936251,  8.936251, ...,  6.      ,  6.      ,\n",
      "         3.      ],\n",
      "       [ 5.93625 ,  8.936251, 11.936251, ...,  6.      ,  9.      ,\n",
      "         6.      ],\n",
      "       ...,\n",
      "       [ 3.      ,  6.      ,  6.      , ...,  8.936251,  8.936251,\n",
      "         5.93625 ],\n",
      "       [ 3.      ,  6.      ,  9.      , ...,  8.936251, 11.936251,\n",
      "         8.936251],\n",
      "       [ 3.      ,  3.      ,  6.      , ...,  5.93625 ,  8.936251,\n",
      "         8.936251]], dtype=float32), 'noise': tensor([[1.4913]], grad_fn=<AddBackward0>), 'like': 0}\n",
      "steps 1.3485968208390289\n",
      "torch.Size([2704, 15])\n",
      "0\n",
      "5\n",
      "Iter 1/50 - Loss: 1.665\n",
      "Iter 2/50 - Loss: 1.651\n",
      "Iter 3/50 - Loss: 1.645\n",
      "Iter 4/50 - Loss: 1.640\n",
      "not enough values to unpack (expected 3, got 2)\n",
      "here\n",
      "(2704, 15)\n",
      "{'sigma_u': array([[0.12018327, 0.1636502 ],\n",
      "       [0.1636502 , 0.17806289]]), 'cov': array([[ 5.970946,  5.970946,  5.970946, ...,  3.      ,  3.      ,\n",
      "         6.      ],\n",
      "       [ 5.970946,  8.970945,  8.970945, ...,  6.      ,  3.      ,\n",
      "         6.      ],\n",
      "       [ 5.970946,  8.970945, 11.970945, ...,  9.      ,  6.      ,\n",
      "         9.      ],\n",
      "       ...,\n",
      "       [ 3.      ,  6.      ,  9.      , ..., 11.970945,  8.970945,\n",
      "         8.970945],\n",
      "       [ 3.      ,  3.      ,  6.      , ...,  8.970945,  8.970945,\n",
      "         8.970945],\n",
      "       [ 6.      ,  6.      ,  9.      , ...,  8.970945,  8.970945,\n",
      "        11.970945]], dtype=float32), 'noise': tensor([[1.4258]], grad_fn=<AddBackward0>), 'like': 0}\n",
      "steps 1.319816681950623\n",
      "torch.Size([3612, 15])\n",
      "0\n",
      "5\n",
      "Iter 1/50 - Loss: 1.634\n",
      "Iter 2/50 - Loss: 1.626\n",
      "Iter 3/50 - Loss: 1.619\n",
      "Iter 4/50 - Loss: 1.615\n",
      "Iter 5/50 - Loss: 1.592\n",
      "Iter 6/50 - Loss: 1.576\n",
      "not enough values to unpack (expected 3, got 2)\n",
      "here\n",
      "(3612, 15)\n",
      "{'sigma_u': array([[0.16475528, 0.13970296],\n",
      "       [0.13970296, 0.08711639]]), 'cov': array([[ 5.9724655,  5.9724655,  5.9724655, ...,  6.       ,  6.       ,\n",
      "         6.       ],\n",
      "       [ 5.9724655,  8.9724655,  8.9724655, ...,  6.       ,  6.       ,\n",
      "         6.       ],\n",
      "       [ 5.9724655,  8.9724655, 11.9724655, ...,  9.       ,  9.       ,\n",
      "         9.       ],\n",
      "       ...,\n",
      "       [ 6.       ,  6.       ,  9.       , ..., 11.9724655, 11.9724655,\n",
      "        11.9724655],\n",
      "       [ 6.       ,  6.       ,  9.       , ..., 11.9724655, 11.9724655,\n",
      "        11.9724655],\n",
      "       [ 6.       ,  6.       ,  9.       , ..., 11.9724655, 11.9724655,\n",
      "        11.9724655]], dtype=float32), 'noise': tensor([[1.5356]], grad_fn=<AddBackward0>), 'like': 0}\n",
      "steps 1.3261098022342348\n",
      "torch.Size([4499, 15])\n",
      "0\n",
      "5\n",
      "Iter 1/50 - Loss: 1.632\n",
      "Iter 2/50 - Loss: 1.621\n",
      "Iter 3/50 - Loss: 1.616\n",
      "Iter 4/50 - Loss: 1.612\n",
      "Iter 5/50 - Loss: 1.578\n",
      "Iter 6/50 - Loss: 1.522\n",
      "Iter 7/50 - Loss: 1.492\n",
      "Iter 8/50 - Loss: 1.476\n",
      "not enough values to unpack (expected 3, got 2)\n",
      "here\n",
      "(4499, 15)\n",
      "{'sigma_u': array([[0.19680706, 0.18547809],\n",
      "       [0.18547809, 0.10275975]]), 'cov': array([[ 5.928611,  5.928611,  5.928611, ...,  3.      ,  3.      ,\n",
      "         3.      ],\n",
      "       [ 5.928611,  8.928611,  8.928611, ...,  6.      ,  6.      ,\n",
      "         6.      ],\n",
      "       [ 5.928611,  8.928611, 11.928611, ...,  6.      ,  9.      ,\n",
      "         9.      ],\n",
      "       ...,\n",
      "       [ 3.      ,  6.      ,  6.      , ...,  8.928611,  8.928611,\n",
      "         8.928611],\n",
      "       [ 3.      ,  6.      ,  9.      , ...,  8.928611, 11.928611,\n",
      "        11.928611],\n",
      "       [ 3.      ,  6.      ,  9.      , ...,  8.928611, 11.928611,\n",
      "        11.928611]], dtype=float32), 'noise': tensor([[1.6156]], grad_fn=<AddBackward0>), 'like': 0}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Downloads/pooling_results/pooling/case_three/population_size_32_update_days_7_short_static_sim_0_alltests.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6b02c296d955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pooling'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'case_three'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../../Downloads/distributions/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../../Downloads/pooling_results/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-98dcd2e43218>\u001b[0m in \u001b[0;36mrun_many\u001b[0;34m(algo_type, cases, sim_start, sim_end, update_time, dist_root, write_directory)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m#return experiment,personal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}{}/population_size_{}_update_days_{}_{}_static_sim_{}_alltests.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}{}/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malgo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpop_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'short'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'gids'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'regrets'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'history'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mto_save\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Downloads/pooling_results/pooling/case_three/population_size_32_update_days_7_short_static_sim_0_alltests.pkl'"
     ]
    }
   ],
   "source": [
    "e,p = run_many('pooling',['case_three'],0,1,7,'../../Downloads/distributions/','../../Downloads/pooling_results/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.28613145**.5*0.02591053**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7393274299268684\n",
      "torch.Size([889, 15])\n",
      "Iter 1/50 - Loss: 1.629\n",
      "Iter 2/50 - Loss: 1.620\n",
      "Iter 3/50 - Loss: 1.617\n",
      "Iter 4/50 - Loss: 1.613\n",
      "not enough values to unpack (expected 3, got 2)\n",
      "here\n",
      "(889, 15)\n",
      "{'sigma_u': array([[0.10478885, 0.16110607],\n",
      "       [0.16110607, 0.19790182]]), 'cov': array([[ 2.9804785,  2.9804785,  2.9804785, ...,  3.       ,  3.       ,\n",
      "         3.       ],\n",
      "       [ 2.9804785,  2.9804785,  2.9804785, ...,  3.       ,  3.       ,\n",
      "         3.       ],\n",
      "       [ 2.9804785,  2.9804785,  5.9804783, ...,  3.       ,  6.       ,\n",
      "         6.       ],\n",
      "       ...,\n",
      "       [ 3.       ,  3.       ,  3.       , ...,  5.9804783,  5.9804783,\n",
      "         5.9804783],\n",
      "       [ 3.       ,  3.       ,  6.       , ...,  5.9804783, 11.980478 ,\n",
      "        11.980478 ],\n",
      "       [ 3.       ,  3.       ,  6.       , ...,  5.9804783, 11.980478 ,\n",
      "        11.980478 ]], dtype=float32), 'noise': tensor([[1.4182]], grad_fn=<AddBackward0>), 'like': 0}\n",
      "torch.Size([1776, 15])\n",
      "Iter 1/50 - Loss: 1.622\n",
      "Iter 2/50 - Loss: 1.609\n",
      "Iter 3/50 - Loss: 1.606\n",
      "Iter 4/50 - Loss: 1.603\n",
      "not enough values to unpack (expected 3, got 2)\n",
      "here\n",
      "(1776, 15)\n",
      "{'sigma_u': array([[0.11130455, 0.16348442],\n",
      "       [0.16348442, 0.19149862]]), 'cov': array([[ 2.9758344,  2.9758344,  2.9758344, ...,  3.       ,  3.       ,\n",
      "         3.       ],\n",
      "       [ 2.9758344,  2.9758344,  2.9758344, ...,  3.       ,  3.       ,\n",
      "         3.       ],\n",
      "       [ 2.9758344,  2.9758344,  5.9758344, ...,  6.       ,  6.       ,\n",
      "         6.       ],\n",
      "       ...,\n",
      "       [ 3.       ,  3.       ,  6.       , ..., 11.975834 , 11.975834 ,\n",
      "        11.975834 ],\n",
      "       [ 3.       ,  3.       ,  6.       , ..., 11.975834 , 14.975834 ,\n",
      "        11.975834 ],\n",
      "       [ 3.       ,  3.       ,  6.       , ..., 11.975834 , 11.975834 ,\n",
      "        11.975834 ]], dtype=float32), 'noise': tensor([[1.4213]], grad_fn=<AddBackward0>), 'like': 0}\n",
      "torch.Size([2686, 15])\n",
      "Iter 1/50 - Loss: 1.623\n",
      "Iter 2/50 - Loss: 1.612\n",
      "Iter 3/50 - Loss: 1.607\n",
      "Iter 4/50 - Loss: 1.604\n",
      "not enough values to unpack (expected 3, got 2)\n",
      "here\n",
      "(2686, 15)\n",
      "{'sigma_u': array([[0.11472286, 0.15876968],\n",
      "       [0.15876968, 0.17680739]]), 'cov': array([[ 2.973991,  2.973991,  2.973991, ...,  3.      ,  3.      ,\n",
      "         3.      ],\n",
      "       [ 2.973991,  2.973991,  2.973991, ...,  3.      ,  3.      ,\n",
      "         3.      ],\n",
      "       [ 2.973991,  2.973991,  5.973991, ...,  6.      ,  3.      ,\n",
      "         6.      ],\n",
      "       ...,\n",
      "       [ 3.      ,  3.      ,  6.      , ..., 14.97399 , 11.97399 ,\n",
      "        11.97399 ],\n",
      "       [ 3.      ,  3.      ,  3.      , ..., 11.97399 , 11.97399 ,\n",
      "         8.97399 ],\n",
      "       [ 3.      ,  3.      ,  6.      , ..., 11.97399 ,  8.97399 ,\n",
      "        11.97399 ]], dtype=float32), 'noise': tensor([[1.4192]], grad_fn=<AddBackward0>), 'like': 0}\n",
      "torch.Size([3596, 15])\n",
      "Iter 1/50 - Loss: 1.622\n",
      "Iter 2/50 - Loss: 1.613\n",
      "Iter 3/50 - Loss: 1.607\n",
      "Iter 4/50 - Loss: 1.604\n",
      "Iter 5/50 - Loss: 1.578\n",
      "not enough values to unpack (expected 3, got 2)\n",
      "here\n",
      "(3596, 15)\n",
      "{'sigma_u': array([[0.16388142, 0.17472856],\n",
      "       [0.17472856, 0.12912562]]), 'cov': array([[ 2.9435499,  2.9435499,  2.9435499, ...,  3.       ,  3.       ,\n",
      "         3.       ],\n",
      "       [ 2.9435499,  2.9435499,  2.9435499, ...,  3.       ,  3.       ,\n",
      "         3.       ],\n",
      "       [ 2.9435499,  2.9435499,  5.94355  , ...,  6.       ,  6.       ,\n",
      "         6.       ],\n",
      "       ...,\n",
      "       [ 3.       ,  3.       ,  6.       , ..., 11.94355  ,  8.94355  ,\n",
      "         8.94355  ],\n",
      "       [ 3.       ,  3.       ,  6.       , ...,  8.94355  ,  8.94355  ,\n",
      "         8.94355  ],\n",
      "       [ 3.       ,  3.       ,  6.       , ...,  8.94355  ,  8.94355  ,\n",
      "         8.94355  ]], dtype=float32), 'noise': tensor([[1.4753]], grad_fn=<AddBackward0>), 'like': 0}\n",
      "torch.Size([4486, 15])\n",
      "Iter 1/50 - Loss: 1.616\n",
      "Iter 2/50 - Loss: 1.607\n",
      "Iter 3/50 - Loss: 1.602\n",
      "Iter 4/50 - Loss: 1.599\n",
      "not enough values to unpack (expected 3, got 2)\n",
      "here\n",
      "(4486, 15)\n",
      "{'sigma_u': array([[0.12716894, 0.15816748],\n",
      "       [0.15816748, 0.15885973]]), 'cov': array([[ 2.9696937,  2.9696937,  2.9696937, ...,  3.       ,  3.       ,\n",
      "         3.       ],\n",
      "       [ 2.9696937,  2.9696937,  2.9696937, ...,  3.       ,  3.       ,\n",
      "         3.       ],\n",
      "       [ 2.9696937,  2.9696937,  5.9696937, ...,  3.       ,  6.       ,\n",
      "         6.       ],\n",
      "       ...,\n",
      "       [ 3.       ,  3.       ,  3.       , ...,  8.969694 ,  5.9696937,\n",
      "         5.9696937],\n",
      "       [ 3.       ,  3.       ,  6.       , ...,  5.9696937, 11.969694 ,\n",
      "        11.969694 ],\n",
      "       [ 3.       ,  3.       ,  6.       , ...,  5.9696937, 11.969694 ,\n",
      "        11.969694 ]], dtype=float32), 'noise': tensor([[1.4165]], grad_fn=<AddBackward0>), 'like': 0}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Downloads/pooling_results/pooling/case_one/population_size_32_update_days_7_short_static_sim_0_alltests.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-850409a05047>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pooling'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'case_one'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../../Downloads/distributions/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../../Downloads/pooling_results/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-98dcd2e43218>\u001b[0m in \u001b[0;36mrun_many\u001b[0;34m(algo_type, cases, sim_start, sim_end, update_time, dist_root, write_directory)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m#return experiment,personal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}{}/population_size_{}_update_days_{}_{}_static_sim_{}_alltests.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}{}/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malgo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpop_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'short'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'gids'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'regrets'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'history'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mto_save\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Downloads/pooling_results/pooling/case_one/population_size_32_update_days_7_short_static_sim_0_alltests.pkl'"
     ]
    }
   ],
   "source": [
    "e,p = run_many('pooling',['case_one'],0,1,7,'../../Downloads/distributions/','../../Downloads/pooling_results/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
