{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append('../models')\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import sim_functions_cleaner  as sf\n",
    "import operator\n",
    "import study\n",
    "import time as time_module\n",
    "\n",
    "import TS_personal_params_pooled as pp\n",
    "import TS_global_params_pooled as gtp\n",
    "from numpy.random import uniform\n",
    "\n",
    "#sys.path.append('../simulation')\n",
    "import TS_fancy_pooled \n",
    "import TS\n",
    "#import TS_fancy_pooled \n",
    "import eta\n",
    "import pooling_bandits as pb\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import feature_transformations as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import muppy\n",
    "all_objects = muppy.get_objects()\n",
    "from pympler import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for each person generate dates\n",
    "##easiest to take from original data, this might best mimic actual situation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root =  '../../../../Volumes/dav/HeartSteps/pooling_rl_shared_data/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_directory = '../temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_policy_params_TS(experiment,update_period,standardize=False):\n",
    "    #,'location_1','location_2','location_3'\n",
    "    #'continuous_temp',\n",
    "    global_p =gtp.TS_global_params(21,baseline_keys=['pretreatment','weather','dow','tod','location'],psi_features=[0,6], responsivity_keys= ['pretreatment','weather','dow','tod','location'])\n",
    "    personal_p = pp.TS_personal_params()\n",
    "    #global_p =gtp.TS_global_params(10,context_dimension)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #global_p.mu_dimension = 64\n",
    "\n",
    "    global_p.kdim =24\n",
    "    #194\n",
    "    global_p.baseline_indices = [i for i in range(24)]\n",
    "    #[i for i in range(192)]\n",
    "    #[0,1,2,3,4,5,6]\n",
    "    global_p.psi_indices =[0,6]\n",
    "    #[0,64]\n",
    "    global_p.user_id_index =21\n",
    " \n",
    "    global_p.psi_features =[0,6]\n",
    "    #[0,64]\n",
    "    \n",
    "    global_p.update_period = update_period\n",
    "    \n",
    "    global_p.standardize = standardize\n",
    "    #print(type(personal_p))\n",
    "    \n",
    "    for person in experiment.population.keys():\n",
    "        experiment.population[person].root = '../../regal/murphy_lab/pooling/distributions/'\n",
    "        initial_context = [0 for i in range(global_p.theta_dim)]\n",
    "        personal_p.mus0[person]= global_p.get_mu0(initial_context)\n",
    "        personal_p.mus1[person]= global_p.get_mu1(global_p.num_baseline_features)\n",
    "        personal_p.mus2[person]= global_p.get_mu2(global_p.num_responsivity_features)\n",
    "        \n",
    "        personal_p.sigmas0[person]= global_p.get_asigma(len( personal_p.mus0[person]))\n",
    "        personal_p.sigmas1[person]= global_p.get_asigma(global_p.num_baseline_features+1)\n",
    "        personal_p.sigmas2[person]= global_p.get_asigma( global_p.num_responsivity_features+1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        personal_p.batch[person]=[[] for i in range(len(experiment.person_to_time[person]))]\n",
    "        personal_p.batch_index[person]=0\n",
    "        \n",
    "        #personal_p.etas[person]=eta.eta()\n",
    "        \n",
    "        personal_p.last_update[person]=experiment.person_to_time[person][0]\n",
    "        \n",
    "        \n",
    "    return global_p ,personal_p     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_reward(beta,states):\n",
    "    return np.dot(beta,states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_kind_of_simulation(experiment,policy=None,personal_policy_params=None,global_policy_params=None,which_gen=None,tf = None):\n",
    "    #write_directory = '../../murphy_lab/lab/pooling/temp'\n",
    "    experiment.last_update_day=experiment.study_days[0]\n",
    "    for time in experiment.study_days:\n",
    "        \n",
    "        #if time> experiment.study_days[0]:\n",
    "        #history  = pb.make_history(experiment)\n",
    "        if time==experiment.last_update_day+pd.DateOffset(days=global_policy_params.update_period):\n",
    "            experiment.last_update_day=time\n",
    "            #print('Global update', time,global_policy_params.decision_times, file=open('updates_{}_{}.txt'.format(len(experiment.population),global_policy_params.update_period), 'a'))\n",
    "            if global_policy_params.decision_times>2:\n",
    "                global_policy_params.last_global_update_time=time\n",
    "              \n",
    "        tod = sf.get_time_of_day(time)\n",
    "        dow = sf.get_day_of_week(time)\n",
    "        if time==experiment.study_days[0]:\n",
    "            print('init weather')\n",
    "            weather = tf.get_weather_prior(tod,time.month,seed=experiment.rando_gen)\n",
    "            temperature = tf.continuous_temperature(weather)\n",
    "        elif time.hour in experiment.weather_update_hours and time.minute==0:\n",
    "            weather = tf.get_next_weather(str(tod),str(time.month),weather,seed=experiment.rando_gen)\n",
    "            temperature = tf.continuous_temperature(weather)\n",
    "            ##location depends on person \n",
    "            \n",
    "        for person in experiment.dates_to_people[time]:\n",
    "                dt=False\n",
    "                action = 0 \n",
    "                prob=0\n",
    "                #1\n",
    "                ##for every active person update person specific aspects of their context\n",
    "                participant = experiment.population[person]\n",
    "                if time==participant.last_update_day+pd.DateOffset(days=global_policy_params.update_period):\n",
    "                    \n",
    "                    history = participant.history\n",
    "                    \n",
    "                    #print(participant.pid)\n",
    "                    #print('updated')\n",
    "                    #print(participant.last_update_day)\n",
    "                    #print(history)\n",
    "                    #return {participant.pid:history}\n",
    "                    temp_hist = tf.get_history_decision_time_avail_single({participant.pid:history},time)\n",
    "                    \n",
    "                    temp_hist= tf.history_semi_continuous(temp_hist,global_policy_params)\n",
    "                    #sf.get_data_for_txt_effect_u\n",
    "                    \n",
    "                    context,steps,probs,actions= tf.get_form_TS(temp_hist)\n",
    "                    #sf.get_data_for_txt_effect_update(history,global_policy_params)\n",
    "                       \n",
    "                    #phi = get_phi(context,probs,actions,[i for i in range(len(context[0]))],[i for i in range(len(context[0]))])\n",
    "                    \n",
    "                    temp = TS.policy_update_ts_new( context,steps,probs,actions,global_policy_params.sigma,\\\n",
    "                                               personal_policy_params.mus1[participant.pid],\\\n",
    "                                               personal_policy_params.sigmas1[participant.pid],\\\n",
    "                                               personal_policy_params.mus2[participant.pid],\\\n",
    "                                               personal_policy_params.sigmas2[participant.pid],\n",
    "                                                \n",
    "                                              )\n",
    "                    mu_beta = temp[0]\n",
    "                    Sigma_beta = temp[1]\n",
    "                    personal_policy_params.update_mus(participant.pid,mu_beta,2)\n",
    "                    personal_policy_params.update_sigmas(participant.pid,Sigma_beta,2)\n",
    "                    participant.last_update_day=time\n",
    "                    history = None\n",
    "    \n",
    "                #update global context variables\n",
    "                participant.set_tod(tod)\n",
    "                participant.set_dow(dow)\n",
    "                participant.set_wea(weather)\n",
    "                \n",
    "                #random.seed(participant.pid)\n",
    "                availability = (participant.rando_gen.uniform() < 0.8)\n",
    "                participant.set_available(availability)\n",
    "                \n",
    "                if time == participant.times[0]:\n",
    "                    #get first location \n",
    "                    location = tf.get_location_prior(str(participant.gid),str(tod),str(dow),seed=participant.rando_gen)\n",
    "                    participant.set_inaction_duration(0)\n",
    "                    participant.set_action_duration(0)\n",
    "                   \n",
    "                    \n",
    "                    \n",
    "                if time <= participant.times[0]:\n",
    "                    steps_last_time_period = 0  \n",
    "                    \n",
    "                    ##set first pre-treatment, yesterday step count, variation and dosage\n",
    "                else:\n",
    "                    \n",
    "                    if time.hour==0 and time.minute==0:\n",
    "                        participant.current_day_counter=participant.current_day_counter+1\n",
    "                    \n",
    "                    #print(time)\n",
    "                    steps_last_time_period = participant.steps\n",
    "                \n",
    "                 \n",
    "\n",
    "                if time.hour in experiment.location_update_hours and time.minute==0:\n",
    "                    location = tf.get_next_location(participant.gid,tod,dow,participant.get_loc(),seed=participant.rando_gen)\n",
    "                \n",
    "    \n",
    "                \n",
    "                participant.set_loc(location)\n",
    "                \n",
    "\n",
    "                prob = -1\n",
    "                add=None\n",
    "                optimal_action = -1\n",
    "                optimal_reward = -100\n",
    "                if time in participant.decision_times:\n",
    "                                        #print(personal_policy_params.batch_index[participant.pid])\n",
    "                    \n",
    "                    \n",
    "                    ##if we have made no global updates\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    dt=True\n",
    "                    action=0\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    \n",
    "                    if policy==None:\n",
    "                        action = sf.get_action(policy)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    elif policy=='TS':\n",
    "                 \n",
    "                        #,int(location==1),int(location==2),int(location==3)\n",
    "                    #temperature,\n",
    "                    #sf.get_pretreatment()\n",
    "                        z=np.array([1,tod,dow,weather,sf.get_pretreatment(participant.steps),location])\n",
    "                        \n",
    "                        \n",
    "                        prob = TS.prob_cal_ts(z,0,personal_policy_params.mus2[participant.pid],personal_policy_params.sigmas2[participant.pid],global_policy_params,seed=experiment.rando_gen)\n",
    "                        \n",
    "                        #print('prob {}'.format(prob))\n",
    "                        #random.seed(participant.pid)\n",
    "                        action = int(experiment.rando_gen.uniform() < prob)\n",
    "                            \n",
    "\n",
    "                    \n",
    "                        if availability:\n",
    "                    \n",
    "\n",
    "                   \n",
    "                    \n",
    "                    \n",
    "                   \n",
    "                    \n",
    "                            context = [action,participant.gid,tod,dow,weather,sf.get_pretreatment(participant.steps),location,\\\n",
    "                0,0,0]\n",
    "                    \n",
    "                        \n",
    "                            steps = tf.get_steps_action(context,seed=participant.rando_gen)\n",
    "                        \n",
    "                        \n",
    "                            add = sf.get_add_no_action(z,experiment.beta,participant.Z)\n",
    "                   \n",
    "                        \n",
    "                            participant.steps =  steps+(action*add)\n",
    "                        \n",
    "                            optimal_reward = get_optimal_reward(experiment.beta,z)\n",
    "                            optimal_action = int(optimal_reward>=0)\n",
    "                        \n",
    "                    else:\n",
    "                   \n",
    "                        steps = tf.get_steps_no_action(participant.gid,tod,dow,location,weather,sf.get_pretreatment(steps_last_time_period),seed=participant.rando_gen)\n",
    "                        participant.steps = steps\n",
    "\n",
    "                \n",
    "\n",
    "                    global_policy_params.decision_times =   global_policy_params.decision_times+1\n",
    "                \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    #participant.steps_last_time_period = participant.steps\n",
    "                        steps = tf.get_steps_no_action(participant.gid,tod,dow,location,weather,sf.get_pretreatment(steps_last_time_period),seed=participant.rando_gen)\n",
    "                        participant.steps = steps     \n",
    "                \n",
    "                ##history:\n",
    "              \n",
    "                 ##history:\n",
    "                context_dict =  {'steps':participant.steps,'add':add,'action':action,'weather':weather,'location':location,'location_1':int(location==1),\\\n",
    "                    'ltps':steps_last_time_period,'location_2':int(location==2),'location_3':int(location==3),\\\n",
    "                        'study_day':participant.current_day_counter,\\\n",
    "                                 'temperature':temperature,'decision_time':dt,\\\n",
    "                                 'time':time,'avail':availability,'prob':prob,\\\n",
    "                                 'dow':dow,'tod':tod,\\\n",
    "                                 'pretreatment':sf.get_pretreatment(steps_last_time_period),\\\n",
    "                                'optimal_reward':optimal_reward,'optimal_action':optimal_action}\n",
    "                participant.history[time]=context_dict\n",
    "\n",
    "\n",
    "                #if global_policy_params.decision_times%100==0:\n",
    "                   # my_directory = '{}/pop_size_{}_update_{}_study_length_{}/participant_{}'.format(global_policy_params.write_directory,participant.pid,experiment.study_length,len(experiment.population),global_policy_params.update_period)\n",
    "                    #if not os.path.exists(my_directory):\n",
    "                     #   os.makedirs(my_directory)\n",
    "                    #with open('{}/history_{}.pkl'.format(my_directory,global_policy_params.decision_times),'wb') as f:\n",
    "                     #   pickle.dump(participant.history,f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_save(exp):\n",
    "    to_save  = {}\n",
    "    for pid,pdata in exp.population.items():\n",
    "        for time,context in pdata.history.items():\n",
    "            key = '{}-{}-{}'.format(pid,time,pdata.gid)\n",
    "            to_save[key]=context\n",
    "    return to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size=32\n",
    "experiment = study.study('../../Downloads/distributions/',pop_size,'short','case_one',sim_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "glob,personal = initialize_policy_params_TS(experiment,1,standardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_trans = tf.feature_transformation('../../Downloads/distributions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weather\n"
     ]
    }
   ],
   "source": [
    "start = time_module.time()\n",
    "hist = new_kind_of_simulation(experiment,'TS',personal,glob,tf=feat_trans)\n",
    "end = time_module.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal.mus2[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_steps  = []\n",
    "for p in experiment.population.values():\n",
    "    all_steps.extend([v['add'] for v in p.history.values() if v['action'] and v['decision_time'] and v['add'] is not None])\n",
    "np.array(all_steps).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_steps  = []\n",
    "for p in experiment.population.values():\n",
    "    all_steps.extend([v['steps'] for v in p.history.values() if v['action'] and v['decision_time']])\n",
    "np.array(all_steps).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regret(experiment):\n",
    "    optimal_actions ={}\n",
    "    rewards = {}\n",
    "    \n",
    "    for pid,person in experiment.population.items():\n",
    "        for time,data in person.history.items():\n",
    "            if data['decision_time'] and data['avail']:\n",
    "                key = time\n",
    "                if key not in optimal_actions:\n",
    "                    optimal_actions[key]=[]\n",
    "                if key not in rewards:\n",
    "                    rewards[key]=[]\n",
    "                if data['optimal_action']!=-1:\n",
    "                    optimal_actions[key].append(int(data['action']==data['optimal_action']))\n",
    "                    regret = int(data['action']!=data['optimal_action'])*(abs(data['optimal_reward']-data['steps']))\n",
    "                    rewards[key].append(regret)\n",
    "    return optimal_actions,rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions,regets = get_regret(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_to_plot_actions(actions):\n",
    "    \n",
    "    skeys = sorted(actions.keys())\n",
    "    to_plot = []\n",
    "    for k in skeys:\n",
    "        to_plot.append(sum(actions[k])/len(actions[k]))\n",
    "    return to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aplot =  get_to_plot_actions(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([aplot[i+3] for i in range(0,len(aplot),5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_many():\n",
    "    for case in ['case_one','case_two','case_three']:\n",
    "        #,'case_two','case_three'\n",
    "    #case = 'case_one'\n",
    "        for u in [1,7]:\n",
    "        \n",
    "            all_actions = {}\n",
    "            all_rewards = {}\n",
    "            feat_trans = tf.feature_transformation('../../Downloads/distributions/')\n",
    "        \n",
    "            for sim in range(50):\n",
    "                pop_size=32\n",
    "                experiment = study.study('../../Downloads/distributions/',pop_size,'short',which_gen=case,sim_number=sim)\n",
    "                glob,personal = initialize_policy_params_TS(experiment,u,standardize=False)\n",
    "                hist = new_kind_of_simulation(experiment,'TS',personal,glob,tf=feat_trans)\n",
    "                to_Save = make_to_save(experiment)\n",
    "                actions,rewards = get_regret(experiment)\n",
    "            \n",
    "                for i,a in actions.items():\n",
    "                    if i not in all_actions:\n",
    "                        all_actions[i]=a\n",
    "                    else:\n",
    "                        all_actions[i].extend(a)\n",
    "                for i,a in rewards.items():\n",
    "                    if i not in all_rewards:\n",
    "                        all_rewards[i]=a\n",
    "                    else:\n",
    "                        all_rewards[i].extend(a)\n",
    "            \n",
    "\n",
    "                filename = '{}/results/{}/population_size_{}_update_days_{}_{}_static_sim_{}_weightedseedsnb_coeff.pkl'.format('../../Downloads/pooling_results/personalized/',case,pop_size,u,'short',sim)\n",
    "                with open(filename,'wb') as f:\n",
    "                    pickle.dump(to_Save,f)\n",
    "            filename = '{}/results/{}/population_size_{}_update_days_{}_{}_static_sim_regrets_actions_l_weightedseedsnb_coeff.pkl'.format('../../Downloads/pooling_results/personalized/',case,pop_size,u,'short')\n",
    "            with open(filename,'wb') as f:\n",
    "                pickle.dump({'actions':all_actions,'regrets':all_rewards},f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_many()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_Save = make_to_save(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "    filename = '{}/results/population_size_{}_update_days_{}_{}_static.pkl'.format('../../Downloads/pooling_results/personalized/',pop_size,7,'short')\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(to_Save,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "goods = []\n",
    "bads = []\n",
    "for participant in experiment.population:\n",
    "    if personal.mus2[participant][0]>0 and personal.mus2[participant][1]>0:\n",
    "        goods.append(participant)\n",
    "    else:\n",
    "        bads.append(participant)\n",
    "print(len(goods))\n",
    "print(len(bads))\n",
    "print(len(goods)/(len(goods)+len(bads)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### all_steps  = [\n",
    "]\n",
    "for p in experiment.population.values():\n",
    "    all_steps.extend([v['steps'] for v in p.history.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#o\n",
    "20.15853214263916\n",
    "0.16741123356661955\n",
    "247.8333568572998\n",
    "#faster\n",
    "\n",
    "#with more data\n",
    "159\n",
    "0.0390877393575815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xz = np.ones((23,23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([np.array([1]+[i for i in row]) for row in xz]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
