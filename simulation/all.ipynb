{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append('../models')\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import operator\n",
    "import study\n",
    "import time as time_module\n",
    "\n",
    "import TS_personal_params_pooled as pp\n",
    "import TS_global_params_pooled as gtp\n",
    "from numpy.random import uniform\n",
    "\n",
    "#sys.path.append('../simulation')\n",
    "import TS_fancy_pooled \n",
    "import TS\n",
    "#import TS_fancy_pooled \n",
    "import eta\n",
    "import pooling_bandits as pb\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import feature_transformations as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_policy_params_TS(experiment,update_period,\\\n",
    "                                standardize=False,baseline_features=None,psi_features=None,\\\n",
    "                                responsivity_keys=None,algo_type=None):\n",
    "    #,'location_1','location_2','location_3'\n",
    "    #'continuous_temp',\n",
    "    global_p =gtp.TS_global_params(21,baseline_features=baseline_features,psi_features=psi_features, responsivity_keys= responsivity_keys)\n",
    "    personal_p = pp.TS_personal_params()\n",
    "    #global_p =gtp.TS_global_params(10,context_dimension)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #global_p.mu_dimension = 64\n",
    "\n",
    "    global_p.kdim =24\n",
    "    #194\n",
    "    global_p.baseline_indices = [i for i in range(24)]\n",
    "    #[i for i in range(192)]\n",
    "    #[0,1,2,3,4,5,6]\n",
    "    global_p.psi_indices = [0] + [1+baseline_features.index(j) for j in psi_features] \\\n",
    "    + [len(baseline_features)+1] + [(2+len(baseline_features))+baseline_features.index(j) for j in psi_features]\n",
    "    #[0,64]\n",
    "    global_p.user_id_index =21\n",
    " \n",
    "    global_p.psi_features =psi_features\n",
    "    #[0,64]\n",
    "    \n",
    "    print(global_p.psi_indices )\n",
    "    \n",
    "    global_p.update_period = update_period\n",
    "    \n",
    "    global_p.standardize = standardize\n",
    "    \n",
    "    \n",
    "    \n",
    "    initial_context = [0 for i in range(global_p.theta_dim)]\n",
    "    \n",
    "    global_p.mus0= global_p.get_mu0(initial_context)\n",
    "    #global_p.get_mu0(initial_context)\n",
    "    global_p.mus1= global_p.get_mu1(global_p.num_baseline_features)\n",
    "    global_p.mus2= global_p.get_mu2(global_p.num_responsivity_features)\n",
    "    #np.array([.120,3.3,-.11])\n",
    "    #global_p.get_mu2(global_p.num_responsivity_features)\n",
    "        \n",
    "    #global_p.sigmas0= global_p.get_asigma(len( personal_p.mus0[person]))\n",
    "    global_p.sigmas1= global_p.get_asigma(global_p.num_baseline_features+1)\n",
    "    global_p.sigmas2= global_p.get_asigma( global_p.num_responsivity_features+1)\n",
    "    \n",
    "    #4.83\n",
    "    global_p.mu2_knot = np.array([0]+[0 for i in range(global_p.num_responsivity_features)])\n",
    "    global_p.mu1_knot = np.zeros(global_p.num_baseline_features+1)\n",
    "    global_p.sigma1_knot = np.eye(global_p.num_baseline_features+1)\n",
    "    global_p.sigma2_knot = np.eye(global_p.num_responsivity_features+1)\n",
    "    #print(type(personal_p))\n",
    "    \n",
    "    for person in experiment.population.keys():\n",
    "        #experiment.population[person].root = '../../regal/murphy_lab/pooling/distributions/'\n",
    "        initial_context = [0 for i in range(global_p.theta_dim)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if algo_type!='batch':\n",
    "            personal_p.mus0[person]= global_p.get_mu0(initial_context)\n",
    "            personal_p.mus1[person]= global_p.get_mu1(global_p.num_baseline_features)\n",
    "            personal_p.mus2[person]= global_p.get_mu2(global_p.num_responsivity_features)\n",
    "        \n",
    "            personal_p.sigmas0[person]= global_p.get_asigma(len( personal_p.mus0[person]))\n",
    "            personal_p.sigmas1[person]= global_p.get_asigma(global_p.num_baseline_features+1)\n",
    "            personal_p.sigmas2[person]= global_p.get_asigma( global_p.num_responsivity_features+1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        personal_p.batch[person]=[[] for i in range(len(experiment.person_to_time[person]))]\n",
    "        personal_p.batch_index[person]=0\n",
    "        \n",
    "        #personal_p.etas[person]=eta.eta()\n",
    "        \n",
    "        personal_p.last_update[person]=experiment.person_to_time[person][0]\n",
    "        \n",
    "        \n",
    "    return global_p ,personal_p     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_reward(beta,states,Z):\n",
    "    if Z is None:\n",
    "        \n",
    "        return np.dot(beta,states)\n",
    "    return np.dot(beta,states)+Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size=32\n",
    "experiment = study.study('../../Downloads/distributions/',pop_size,'_short_unstaggered_6','case_three',sim_number=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = ['tod','dow','pretreatment','location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[0, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "glob,person = initialize_policy_params_TS(experiment,7,standardize=False,baseline_features=baseline,psi_features=['tod'],responsivity_keys=baseline,algo_type = None)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_save(exp):\n",
    "    to_save  = {}\n",
    "    for pid,pdata in exp.population.items():\n",
    "        for time,context in pdata.history.items():\n",
    "            \n",
    "            key = '{}-{}-{}'.format(pid,time,pdata.gid)\n",
    "            to_save[key]=context\n",
    "    return to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_kind_of_simulation(experiment,policy=None,personal_policy_params=None,global_policy_params=None,generative_functions=None,which_gen=None,feat_trans = None,algo_type = None):\n",
    "    #write_directory = '../../murphy_lab/lab/pooling/temp'\n",
    "    experiment.last_update_day=experiment.study_days[0]\n",
    "    tod_check = set([])\n",
    "    \n",
    "    \n",
    "    additives = []\n",
    "    \n",
    "    for time in experiment.study_days:\n",
    "        #if time>experiment.study_days[0]+pd.DateOffset(days=3):\n",
    "            #break\n",
    "        #if time> experiment.study_days[0]:\n",
    "        #history  = pb.make_history(experiment)\n",
    "        #time==experiment.last_update_day+pd.DateOffset(days=global_policy_params.update_period)\n",
    "        if time==experiment.last_update_day+pd.DateOffset(days=global_policy_params.update_period):\n",
    "            experiment.last_update_day=time\n",
    "            \n",
    "            #print('Global update', time,global_policy_params.decision_times, file=open('updates_{}_{}.txt'.format(len(experiment.population),global_policy_params.update_period), 'a'))\n",
    "            if global_policy_params.decision_times>2 :\n",
    "                global_policy_params.last_global_update_time=time\n",
    "\n",
    "                \n",
    "                ##Do updates\n",
    "\n",
    "                ##move this elsewhere ? \n",
    "                if algo_type=='batch':\n",
    "                \n",
    "                    temp_hist = feat_trans.get_history_decision_time_avail(experiment,time)\n",
    "                    temp_hist= feat_trans.history_semi_continuous(temp_hist,global_policy_params)\n",
    "                    context,steps,probs,actions= feat_trans.get_form_TS(temp_hist)\n",
    "                \n",
    "                    temp_data = feat_trans.get_phi_from_history_lookups(temp_hist)\n",
    "                    #stepsmean = steps.mean()\n",
    "                    #global_policy_params.mu_theta[0]=stepsmean\n",
    "                    steps = feat_trans.get_RT_o(steps,temp_data[0],global_policy_params.mu_theta,global_policy_params.theta_dim)\n",
    "\n",
    "                    temp = TS.policy_update_ts_new( context,steps,probs,actions,global_policy_params.noise_term,\\\n",
    "                                                  global_policy_params.mu1_knot,\\\n",
    "                                                global_policy_params.sigma1_knot,\\\n",
    "                                                global_policy_params.mu2_knot,\\\n",
    "                                                global_policy_params.sigma2_knot)   \n",
    "                                              \n",
    "                                                 #global_policy_params.mus1,\\\n",
    "                                               #global_policy_params.sigmas1,\\\n",
    "                                               #global_policy_params.mus2,\\\n",
    "                                               #global_policy_params.sigmas2) \n",
    "                    mu_beta = temp[0]\n",
    "                    Sigma_beta = temp[1]\n",
    "                    #print(mu_beta)\n",
    "                    global_policy_params.update_mus(None,mu_beta,2)\n",
    "                    global_policy_params.update_sigmas(None,Sigma_beta,2)\n",
    "                    \n",
    "                elif algo_type == 'pooling':\n",
    "                    temp =feat_trans.get_history_decision_time_avail(experiment,time)\n",
    "                    \n",
    "                    t = feat_trans.history_semi_continuous(temp,global_policy_params)\n",
    "                    history = feat_trans.get_phi_from_history_lookups(t)\n",
    "                    context,steps,probs,actions=  feat_trans.get_form_TS(t)\n",
    "                    temp_data = feat_trans.get_phi_from_history_lookups(t)\n",
    "                    #stepsmean = steps.mean()\n",
    "                    #global_policy_params.mu_theta[0]=stepsmean\n",
    "                    steps = feat_trans.get_RT_o(steps,temp_data[0],global_policy_params.mu_theta,global_policy_params.theta_dim)\n",
    "                    temp = TS.policy_update_ts_new( context,steps,probs,actions,global_policy_params.noise_term,\\\n",
    "                                                  global_policy_params.mu1_knot,\\\n",
    "                                                global_policy_params.sigma1_knot,\\\n",
    "                                                global_policy_params.mu2_knot,\\\n",
    "                                                global_policy_params.sigma2_knot\n",
    "                                               )\n",
    "                \n",
    "                    global_posterior = temp[0]\n",
    "                    global_posterior_sigma = temp[1]\n",
    "                    \n",
    "                \n",
    "        tod = feat_trans.get_time_of_day(time)\n",
    "        dow = feat_trans.get_day_of_week(time)\n",
    "        \n",
    "        \n",
    " \n",
    "        \n",
    "        if time==experiment.study_days[0]:\n",
    "            #print('init weather')\n",
    "            weather = feat_trans.get_weather_prior(tod,time.month,seed=experiment.weather_gen)\n",
    "            #temperature = tf.continuous_temperature(weather)\n",
    "        elif time.hour in experiment.weather_update_hours and time.minute==0:\n",
    "            weather = feat_trans.get_next_weather(str(tod),str(time.month),weather,seed=experiment.weather_gen)\n",
    "            #temperature = tf.continuous_temperature(weather)\n",
    "            ##location depends on person \n",
    "            \n",
    "        for person in experiment.dates_to_people[time]:\n",
    "                dt=int(time in participant.decision_times)\n",
    "                action = 0 \n",
    "                prob=0\n",
    "                #1\n",
    "                ##for every active person update person specific aspects of their context\n",
    "                participant = experiment.population[person]\n",
    "                \n",
    "                \n",
    "                ##Now update\n",
    "                if algo_type=='personalized' and time==participant.last_update_day+pd.DateOffset(days=global_policy_params.update_period):\n",
    "                    history = participant.history\n",
    "                    temp_hist = feat_trans.get_history_decision_time_avail_single({participant.pid:history},time)\n",
    "                    temp_hist= feat_trans.history_semi_continuous(temp_hist,global_policy_params)\n",
    "                    context,steps,probs,actions= feat_trans.get_form_TS(temp_hist)\n",
    "                    temp_data = feat_trans.get_phi_from_history_lookups(temp_hist)\n",
    "                    \n",
    "                    ##global_policy_params mu theta could be personalized instead but doesn't have to be\n",
    "                    steps = feat_trans.get_RT_o(steps,temp_data[0],global_policy_params.mu_theta,global_policy_params.theta_dim)\n",
    "                   \n",
    "                    temp = TS.policy_update_ts_new( context,steps,probs,actions,global_policy_params.noise_term,\\\n",
    "                                                global_policy_params.mu1_knot,\\\n",
    "                                                global_policy_params.sigma1_knot,\\\n",
    "                                                global_policy_params.mu2_knot,\\\n",
    "                                                global_policy_params.sigma2_knot,  \n",
    "                                                #personal_policy_params.mus1[participant.pid],\\\n",
    "                                               #personal_policy_params.sigmas1[participant.pid],\\\n",
    "                                               #personal_policy_params.mus2[participant.pid],\\\n",
    "                                               #personal_policy_params.sigmas2[participant.pid],\n",
    "                                                \n",
    "                                              )\n",
    "                \n",
    "                    mu_beta = temp[0]\n",
    "                    Sigma_beta = temp[1]\n",
    "                    personal_policy_params.update_mus(participant.pid,mu_beta,2)\n",
    "                    personal_policy_params.update_sigmas(participant.pid,Sigma_beta,2)\n",
    "                    participant.last_update_day=time\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                #update global context variables\n",
    "                participant.set_tod(tod)\n",
    "                participant.set_dow(dow)\n",
    "                #participant.set_wea(weather)\n",
    "                \n",
    "            \n",
    "                availability = (participant.rando_gen.uniform() < 0.8)\n",
    "                #print(participant.pid)\n",
    "                #print(availability)\n",
    "                participant.set_available(availability)\n",
    "                \n",
    "                if time == participant.times[0]:\n",
    "                    #get first location \n",
    "                    location = feat_trans.get_location_prior(str(participant.gid),str(tod),str(dow),seed = participant.rando_gen)\n",
    "                    participant.set_inaction_duration(0)\n",
    "                    participant.set_action_duration(0)\n",
    "                    #participant.set_duration(0)\n",
    "                    #participant.set_dosage(0)\n",
    "                    #personal_policy_params.etas[participant.pid]\n",
    "                    \n",
    "                    \n",
    "                if time <= participant.times[0]:\n",
    "                    steps_last_time_period = 0  \n",
    "                    \n",
    "                    ##set first pre-treatment, yesterday step count, variation and dosage\n",
    "                else:\n",
    "                    \n",
    "                    if time.hour==0 and time.minute==0:\n",
    "                        participant.current_day_counter=participant.current_day_counter+1\n",
    "                    \n",
    "                    #print(time)\n",
    "                    steps_last_time_period = participant.steps\n",
    "                \n",
    "                 \n",
    "\n",
    "                if time.hour in experiment.location_update_hours and time.minute==0:\n",
    "                    location = feat_trans.get_next_location(participant.gid,tod,dow,participant.get_loc(),seed =participant.rando_gen)\n",
    "                \n",
    "    \n",
    "                \n",
    "                participant.set_loc(location)\n",
    "              \n",
    "                \n",
    "\n",
    "                prob = -1\n",
    "                add=None\n",
    "                optimal_action = -1\n",
    "                optimal_reward = -100\n",
    "                if time in dt:\n",
    "                     \n",
    "                    \n",
    "                    \n",
    "                    dt=True\n",
    "                    action=0\n",
    "                    \n",
    "                    if policy==None:\n",
    "                        ##default policy now requires own random stream?\n",
    "                        action = 1\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    elif policy=='TS':\n",
    "                        \n",
    "                        \n",
    "                        if 'pretreatment' in global_policy_params.baseline_features:\n",
    "                            to_call = feat_trans.get_pretreatment(steps_last_time_period)\n",
    "                        else:\n",
    "                            to_call = steps_last_time_period\n",
    "                        \n",
    "                        ##want this to be a function\n",
    "                        \n",
    "                        z = [1]\n",
    "                        \n",
    "                        if 'tod' in global_policy_params.baseline_features:\n",
    "                            z.append(tod)\n",
    "                        if 'dow' in global_policy_params.baseline_features:\n",
    "                            z.append(dow)\n",
    "                        if 'preatment' in global_policy_params.baseline_features:\n",
    "                            z.append(to_call)\n",
    "                        if 'location' in global_policy_params.baseline_features:\n",
    "                            z.append(location)\n",
    "                        #print(z)\n",
    "                        \n",
    "                        \n",
    "                        if algo_type=='batch':\n",
    "                            \n",
    "                        \n",
    "                        \n",
    "                            prob = TS.prob_cal_ts(z,0,global_policy_params.mus2,global_policy_params.sigmas2,global_policy_params,seed = experiment.algo_rando_gen)\n",
    "                        #if participant.pid==1:\n",
    "                            \n",
    "                            #print('prob _ {}'.format(prob))\n",
    "                            #print(type(prob))\n",
    "                        elif algo_type=='personalized':\n",
    "                             prob = TS.prob_cal_ts(z,0,personal_policy_params.mus2[participant.pid],personal_policy_params.sigmas2[participant.pid],global_policy_params,seed=experiment.algo_rando_gen)\n",
    "                        \n",
    "                        \n",
    "                        action = int(experiment.algo_rando_gen.uniform() < prob)\n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                    if availability:\n",
    "                    \n",
    "\n",
    "                   \n",
    "                        tod_check.add(tod)\n",
    "                    \n",
    "                   \n",
    "                    \n",
    "                        context = [action,participant.gid,tod,dow,weather,feat_trans.get_pretreatment(steps_last_time_period),location,\\\n",
    "                              0,0,0]\n",
    "                    \n",
    "                        #participant.steps_last_time_period = participant.steps\n",
    "                        #print(sf.get_pretreatment(participant.steps))\n",
    "                        \n",
    "                        steps = feat_trans.get_steps_action(context,seed = participant.rando_gen)\n",
    "                       \n",
    "                        #add = sf.get_add_two(action,z,experiment.beta,participant.Z)\n",
    "                        add = action*(feat_trans.get_add_no_action(z,experiment.beta,participant.Z))\n",
    "                        additives.append([action,add,prob])\n",
    "                        participant.steps = steps+add\n",
    "                        \n",
    "                        ##calculate optimal\n",
    "                        optimal_reward = get_optimal_reward(experiment.beta,z,participant.Z)\n",
    "                        optimal_action = int(optimal_reward>=0)\n",
    "                        \n",
    "                    else:\n",
    "                        #participant.steps_last_time_period = participant.steps\n",
    "                        steps = feat_trans.get_steps_no_action(participant.gid,tod,dow,location,\\\n",
    "                                                       feat_trans.get_pretreatment(steps_last_time_period),weather,seed = participant.rando_gen)\n",
    "                        participant.steps = steps\n",
    "\n",
    "                \n",
    "\n",
    "                    global_policy_params.decision_times =   global_policy_params.decision_times+1\n",
    "\n",
    "                    \n",
    "                else:\n",
    "                    #participant.steps_last_time_period = participant.steps\n",
    "                        steps = feat_trans.get_steps_no_action(participant.gid,tod,dow,location,\\\n",
    "                                                       feat_trans.get_pretreatment(steps_last_time_period),weather,seed = participant.rando_gen)\n",
    "                        participant.steps = steps     \n",
    "                \n",
    "                ##history:\n",
    "                context_dict =  {'steps':participant.steps,'add':add,'action':action,'location':location,'location_1':int(location==1),\\\n",
    "                    'ltps':steps_last_time_period,'location_2':int(location==2),'location_3':int(location==3),\\\n",
    "                        'study_day':participant.current_day_counter,\\\n",
    "                                 'decision_time':dt,\\\n",
    "                                 'time':time,'avail':availability,'prob':prob,\\\n",
    "                                 'dow':dow,'tod':tod,'weather':weather,\\\n",
    "                                 'pretreatment':feat_trans.get_pretreatment(steps_last_time_period),\\\n",
    "                                'optimal_reward':optimal_reward,'optimal_action':optimal_action,\\\n",
    "                                 'mu2':global_policy_params.mus2,'gid':participant.gid}\n",
    "                participant.history[time]=context_dict\n",
    "                #if participant.pid==0 and time in participant.decision_times:\n",
    "                    #print(participant.rando_gen.rand())\n",
    "                    #print(context_dict)\n",
    "                    \n",
    "\n",
    "\n",
    "                #if global_policy_params.decision_times%100==0:\n",
    "                   # my_directory = '{}/pop_size_{}_update_{}_study_length_{}/participant_{}'.format(global_policy_params.write_directory,participant.pid,experiment.study_length,len(experiment.population),global_policy_params.update_period)\n",
    "                    #if not os.path.exists(my_directory):\n",
    "                     #   os.makedirs(my_directory)\n",
    "                    #with open('{}/history_{}.pkl'.format(my_directory,global_policy_params.decision_times),'wb') as f:\n",
    "                     #   pickle.dump(participant.history,f)\n",
    "\n",
    "\n",
    "    return additives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regret(experiment):\n",
    "    optimal_actions ={}\n",
    "    rewards = {}\n",
    "    actions = {}\n",
    "    for pid,person in experiment.population.items():\n",
    "        for time,data in person.history.items():\n",
    "            if data['decision_time'] and data['avail']:\n",
    "                key = time\n",
    "                if key not in optimal_actions:\n",
    "                    optimal_actions[key]=[]\n",
    "                if key not in rewards:\n",
    "                    rewards[key]=[]\n",
    "                if key not in actions:\n",
    "                    actions[key]=[]\n",
    "                if data['optimal_action']!=-1:\n",
    "                    optimal_actions[key].append(int(data['action']==data['optimal_action']))\n",
    "                    regret = int(data['action']!=data['optimal_action'])*(abs(data['optimal_reward']))\n",
    "                    rewards[key].append(regret)\n",
    "                    actions[key].append(data['action'])\n",
    "    return optimal_actions,rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_many(algo_type):\n",
    "    for case in ['case_one','case_two','case_three']:\n",
    "        #,'case_two','case_three'\n",
    "    #case = 'case_one'\n",
    "    \n",
    "    \n",
    "        baseline = ['location']\n",
    "        \n",
    "        \n",
    "        \n",
    "        for u in [7,1]:\n",
    "        \n",
    "            all_actions = {}\n",
    "            all_rewards = {}\n",
    "            feat_trans = tf.feature_transformation('../../Downloads/distributions/')\n",
    "        \n",
    "            for sim in range(50):\n",
    "                pop_size=32\n",
    "                experiment = study.study('../../Downloads/distributions/',pop_size,'_short_unstaggered_6',which_gen=case,sim_number=sim)\n",
    "                experiment.update_beta(set(baseline))\n",
    "                #print('beta')\n",
    "                #print(experiment.beta)\n",
    "                glob,personal = initialize_policy_params_TS(experiment,7,standardize=False,baseline_features=baseline,psi_features=['location'],responsivity_keys=baseline,algo_type =algo_type)\n",
    "  \n",
    "                hist = new_kind_of_simulation(experiment,'TS',personal,glob,feat_trans=feat_trans,algo_type=algo_type)\n",
    "                to_Save = make_to_save(experiment)\n",
    "                actions,rewards = get_regret(experiment)\n",
    "            \n",
    "                for i,a in actions.items():\n",
    "                    if i not in all_actions:\n",
    "                        all_actions[i]=a\n",
    "                    else:\n",
    "                        all_actions[i].extend(a)\n",
    "                for i,a in rewards.items():\n",
    "                    if i not in all_rewards:\n",
    "                        all_rewards[i]=a\n",
    "                    else:\n",
    "                        all_rewards[i].extend(a)\n",
    "            \n",
    "                #return experiment,personal\n",
    "                filename = '{}/results/{}/population_size_{}_update_days_{}_{}_static_sim_{}_locationonly.pkl'.format('../../Downloads/pooling_results/personalized/',case,pop_size,u,'short',sim)\n",
    "                with open(filename,'wb') as f:\n",
    "                    pickle.dump(to_Save,f)\n",
    "            filename = '{}/results/{}/population_size_{}_update_days_{}_{}_static_sim_regrets_actions_l_locationonly.pkl'.format('../../Downloads/pooling_results/personalized/',case,pop_size,u,'short')\n",
    "            with open(filename,'wb') as f:\n",
    "                pickle.dump({'actions':all_actions,'regrets':all_rewards},f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[0, 1, 2, 3]\n",
      "6\n",
      "[0, 1, 2, 3]\n",
      "6\n",
      "[0, 1, 2, 3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-22a72deb6526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'personalized'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-9df067147404>\u001b[0m in \u001b[0;36mrun_many\u001b[0;34m(algo_type)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpersonal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_policy_params_TS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbaseline_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpsi_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'location'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresponsivity_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malgo_type\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0malgo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_kind_of_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'TS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpersonal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeat_trans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat_trans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malgo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mto_Save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_to_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_regret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-8f8e115f2e22>\u001b[0m in \u001b[0;36mnew_kind_of_simulation\u001b[0;34m(experiment, policy, personal_policy_params, global_policy_params, generative_functions, which_gen, feat_trans, algo_type)\u001b[0m\n\u001b[1;32m    242\u001b[0m                         \u001b[0;31m#print(sf.get_pretreatment(participant.steps))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_steps_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparticipant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrando_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                         \u001b[0;31m#add = sf.get_add_two(action,z,experiment.beta,participant.Z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pooling_rl/simulation/feature_transformations.py\u001b[0m in \u001b[0;36mget_steps_action\u001b[0;34m(self, context, seed)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                 \u001b[0mnew_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m                 \u001b[0mnew_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mnew_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "e,p = run_many('personalized')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.51453994,  0.96728508,  0.18451722,  0.40089515, -0.66710587])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
