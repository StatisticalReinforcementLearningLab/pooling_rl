{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "#sys.path\n",
    "#sys.path.append('../../../home00/stomkins/pooling_rl/models')\n",
    "#sys.path.append('../../../home00/stomkins/pooling_rl/simulation')\n",
    "import pandas as pd\n",
    "sys.path\n",
    "sys.path.append('../models')\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import run_gpytorchkernel\n",
    "import operator\n",
    "import study\n",
    "import time as time_module\n",
    "import simple_bandits\n",
    "import TS_personal_params_pooled as pp\n",
    "import TS_global_params_pooled as gtp\n",
    "from numpy.random import uniform\n",
    "import run_gpy\n",
    "#sys.path.append('../simulation')\n",
    "import TS_fancy_pooled\n",
    "import TS\n",
    "#import TS_fancy_pooled\n",
    "import eta\n",
    "import pooling_bandits as pb\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import feature_transformations as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_policy_params_TS(experiment,update_period,\\\n",
    "                                standardize=False,baseline_features=None,psi_features=None,\\\n",
    "                                responsivity_keys=None,algo_type=None):\n",
    "    #,'location_1','location_2','location_3'\n",
    "    #'continuous_temp',\n",
    "    global_p =gtp.TS_global_params(21,baseline_features=baseline_features,psi_features=psi_features, responsivity_keys= responsivity_keys)\n",
    "    personal_p = pp.TS_personal_params()\n",
    "    #global_p =gtp.TS_global_params(10,context_dimension)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #global_p.mu_dimension = 64\n",
    "    \n",
    "    global_p.kdim =24\n",
    "    #194\n",
    "    global_p.baseline_indices = [i for i in range(3+ 3*len(baseline_features))]\n",
    "    #[i for i in range(192)]\n",
    "    #[0,1,2,3,4,5,6]\n",
    "    global_p.psi_indices = [0] + [1+baseline_features.index(j) for j in psi_features] \\\n",
    "    + [len(baseline_features)+1] + [(2+len(baseline_features))+baseline_features.index(j) for j in psi_features]\n",
    "    #[0,64]\n",
    "    global_p.user_id_index =0\n",
    "    \n",
    "    global_p.psi_features =psi_features\n",
    "    #[0,64]\n",
    "    \n",
    "    #print(global_p.psi_indices )\n",
    "    \n",
    "    global_p.update_period = update_period\n",
    "    \n",
    "    global_p.standardize = standardize\n",
    "    \n",
    "    \n",
    "    \n",
    "    initial_context = [0 for i in range(global_p.theta_dim)]\n",
    "    \n",
    "    global_p.mus0= global_p.get_mu0(initial_context)\n",
    "    #global_p.get_mu0(initial_context)\n",
    "    global_p.mus1= global_p.get_mu1(global_p.num_baseline_features)\n",
    "    global_p.mus2= global_p.get_mu2(global_p.num_responsivity_features)\n",
    "    #np.array([.120,3.3,-.11])\n",
    "    #global_p.get_mu2(global_p.num_responsivity_features)\n",
    "    \n",
    "    #global_p.sigmas0= global_p.get_asigma(len( personal_p.mus0[person]))\n",
    "    global_p.sigmas1= global_p.get_asigma(global_p.num_baseline_features+1)\n",
    "    global_p.sigmas2= global_p.get_asigma( global_p.num_responsivity_features+1)\n",
    "    \n",
    "    #4.83\n",
    "    global_p.mu2_knot = np.array([0]+[0 for i in range(global_p.num_responsivity_features)])\n",
    "    global_p.mu1_knot = np.zeros(global_p.num_baseline_features+1)\n",
    "    global_p.sigma1_knot = np.eye(global_p.num_baseline_features+1)\n",
    "    global_p.sigma2_knot = np.eye(global_p.num_responsivity_features+1)\n",
    "    #print(type(personal_p))\n",
    "    \n",
    "    for person in experiment.population.keys():\n",
    "        #experiment.population[person].root = '../../regal/murphy_lab/pooling/distributions/'\n",
    "        initial_context = [0 for i in range(global_p.theta_dim)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if algo_type!='batch':\n",
    "            personal_p.mus0[person]= global_p.get_mu0(initial_context)\n",
    "            personal_p.mus1[person]= global_p.get_mu1(global_p.num_baseline_features)\n",
    "            personal_p.mus2[person]= global_p.get_mu2(global_p.num_responsivity_features)\n",
    "            \n",
    "            personal_p.sigmas0[person]= global_p.get_asigma(len( personal_p.mus0[person]))\n",
    "            personal_p.sigmas1[person]= global_p.get_asigma(global_p.num_baseline_features+1)\n",
    "            personal_p.sigmas2[person]= global_p.get_asigma( global_p.num_responsivity_features+1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        personal_p.batch[person]=[[] for i in range(len(experiment.person_to_time[person]))]\n",
    "        personal_p.batch_index[person]=0\n",
    "        \n",
    "        #personal_p.etas[person]=eta.eta()\n",
    "        \n",
    "        personal_p.last_update[person]=experiment.person_to_time[person][0]\n",
    "\n",
    "\n",
    "    return global_p ,personal_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_reward(beta,states,Z):\n",
    "    if Z is None:\n",
    "        \n",
    "        return np.dot(beta,states)\n",
    "    return np.dot(beta,states)+Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pop_size=32\n",
    "#experiment = study.study('../../Downloads/distributions/',pop_size,'_short_unstaggered_6','case_three',sim_number=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = ['tod','dow','pretreatment','location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glob,person = initialize_policy_params_TS(experiment,7,standardize=False,baseline_features=baseline,psi_features=[],responsivity_keys=baseline,algo_type = None)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glob.psi_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_save(exp):\n",
    "    to_save  = {}\n",
    "    for pid,pdata in exp.population.items():\n",
    "        for time,context in pdata.history.items():\n",
    "            \n",
    "            key = '{}-{}-{}'.format(pid,time,pdata.gid)\n",
    "            to_save[key]=context\n",
    "    return to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_kind_of_simulation(experiment,policy=None,personal_policy_params=None,global_policy_params=None,generative_functions=None,which_gen=None,feat_trans = None,algo_type = None,case=None,sim_num=None):\n",
    "    #write_directory = '../../murphy_lab/lab/pooling/temp'\n",
    "    experiment.last_update_day=experiment.study_days[0]\n",
    "    tod_check = set([])\n",
    "    \n",
    "    \n",
    "    additives = []\n",
    "    \n",
    "    for time in experiment.study_days:\n",
    "\n",
    "        if time==experiment.last_update_day+pd.DateOffset(days=global_policy_params.update_period):\n",
    "            experiment.last_update_day=time\n",
    "\n",
    "            if global_policy_params.decision_times>2:\n",
    "                global_policy_params.last_global_update_time=time\n",
    "\n",
    "\n",
    "                if algo_type=='batch' or algo_type=='pooling':\n",
    "\n",
    "                    temp_hist = feat_trans.get_history_decision_time_avail(experiment,time)\n",
    "                    temp_hist= feat_trans.history_semi_continuous(temp_hist,global_policy_params)\n",
    "                    context,steps,probs,actions= feat_trans.get_form_TS(temp_hist)\n",
    "                    #print(steps)\n",
    "                    temp_data = feat_trans.get_phi_from_history_lookups(temp_hist)\n",
    "\n",
    "                    steps = feat_trans.get_RT_o(steps,temp_data[0],global_policy_params.mu_theta,global_policy_params.theta_dim)\n",
    "                    #print(steps)\n",
    "                    print('steps {}'.format(steps.std()))\n",
    "                    temp = TS.policy_update_ts_new( context,steps,probs,actions,global_policy_params.noise_term,\\\n",
    "                                                       global_policy_params.mu1_knot,\\\n",
    "                                                       global_policy_params.sigma1_knot,\\\n",
    "                                                       global_policy_params.mu2_knot,\\\n",
    "                                                       global_policy_params.sigma2_knot)\n",
    "\n",
    "                    mu_beta = temp[0]\n",
    "                    Sigma_beta = temp[1]\n",
    "                    #print(mu_beta)\n",
    "                    if algo_type=='batch':\n",
    "                        global_policy_params.update_mus(None,mu_beta,2)\n",
    "                        global_policy_params.update_sigmas(None,Sigma_beta,2)\n",
    "                    else :\n",
    "                        global_posterior = mu_beta\n",
    "                        global_posterior_sigma = Sigma_beta\n",
    "                        try:\n",
    "                            \n",
    "                            #print(baseline_features)\n",
    "                            #print(temp_data[0][:,5])\n",
    "                            temp_params = run_gpytorchkernel.run(temp_data[0], temp_data[1],steps,global_policy_params)\n",
    "                            #print(temp_data[0].shape)\n",
    "                            print(temp_params)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            print('was error')\n",
    "                           # print('global_info',e, time,global_policy_params.decision_times,'error in running gp',file=open('pooling/{}/updates_global_newbigtest_{}_{}_{}six_weeks_only_onoise_errorscurrent.txt'.format(case,len(experiment.population),global_policy_params.update_period,sim_num), 'a'))\n",
    "                            temp_params={'cov':global_policy_params.cov,'noise':global_policy_params.noise_term,'like':-100333,'sigma_u':global_policy_params.sigma_u}\n",
    "                        inv_term = simple_bandits.get_inv_term(temp_params['cov'],temp_data[0].shape[0],temp_params['noise'])\n",
    "                        global_policy_params.update_params(temp_params)\n",
    "                        global_policy_params.inv_term=inv_term\n",
    "                                    #print(temp_params)\n",
    "                        global_policy_params.history = temp_data\n",
    "                        \n",
    "\n",
    "\n",
    "        tod = feat_trans.get_time_of_day(time)\n",
    "        dow = feat_trans.get_day_of_week(time)\n",
    "\n",
    "        if time==experiment.study_days[0]:\n",
    "                        #print('init weather')\n",
    "            weather = feat_trans.get_weather_prior(tod,time.month,seed=experiment.weather_gen)\n",
    "                            #temperature = tf.continuous_temperature(weather)\n",
    "        elif time.hour in experiment.weather_update_hours and time.minute==0:\n",
    "            weather = feat_trans.get_next_weather(str(tod),str(time.month),weather,seed=experiment.weather_gen)\n",
    "\n",
    "        for person in experiment.dates_to_people[time]:\n",
    "            participant = experiment.population[person]\n",
    "            dt=int(time in participant.decision_times)\n",
    "            action = 0\n",
    "            prob=0\n",
    "\n",
    "\n",
    "            if algo_type=='personalized' and dt and time==participant.last_update_day+pd.DateOffset(days=global_policy_params.update_period):\n",
    "                temp_hist = feat_trans.get_history_decision_time_avail_single({participant.pid:participant.history},time)\n",
    "                temp_hist= feat_trans.history_semi_continuous(temp_hist,global_policy_params)\n",
    "                context,steps,probs,actions= feat_trans.get_form_TS(temp_hist)\n",
    "              \n",
    "                temp_data = feat_trans.get_phi_from_history_lookups(temp_hist)\n",
    "                steps = feat_trans.get_RT_o(steps,temp_data[0],global_policy_params.mu_theta,global_policy_params.theta_dim)\n",
    "               \n",
    "                temp = TS.policy_update_ts_new( context,steps,probs,actions,global_policy_params.noise_term,\\\n",
    "                                                           global_policy_params.mu1_knot,\\\n",
    "                                                           global_policy_params.sigma1_knot,\\\n",
    "                                                           global_policy_params.mu2_knot,\\\n",
    "                                                           global_policy_params.sigma2_knot,\n",
    "                                                           #personal_policy_params.mus1[participant.pid],\\\n",
    "                                                           #personal_policy_params.sigmas1[participant.pid],\\\n",
    "                                                           #personal_policy_params.mus2[participant.pid],\\\n",
    "                                                           #personal_policy_params.sigmas2[participant.pid],\n",
    "                                                           \n",
    "                                                           )\n",
    "                mu_beta = temp[0]\n",
    "                Sigma_beta = temp[1]\n",
    "                personal_policy_params.update_mus(participant.pid,mu_beta,2)\n",
    "                personal_policy_params.update_sigmas(participant.pid,Sigma_beta,2)\n",
    "                participant.last_update_day=time\n",
    "            elif algo_type=='pooling'  and global_policy_params.decision_times>2 and global_policy_params.history!=None and  time==participant.last_update_day+pd.DateOffset(days=global_policy_params.update_period):\n",
    "                history = global_policy_params.history\n",
    "                temp = simple_bandits.calculate_posterior_faster(global_policy_params,\\\n",
    "                                                         participant.pid,participant.current_day_counter,\\\n",
    "                                                         history[0], history[1],history[2] )\n",
    "            \n",
    "                                                         #global_posterior = mu_beta\n",
    "                                                         #global_posterior_sigma = Sigma_beta\n",
    "                personal_policy_params.update_mus(participant.pid,mu_beta,2)\n",
    "                personal_policy_params.update_sigmas(participant.pid,Sigma_beta,2)\n",
    "                participant.last_update_day=time\n",
    "            participant.set_tod(tod)\n",
    "            participant.set_dow(dow)\n",
    "\n",
    "            availability = (participant.rando_gen.uniform() < 0.8)\n",
    "    \n",
    "            participant.set_available(availability)\n",
    "            if time == participant.times[0]:\n",
    "                location = feat_trans.get_location_prior(str(participant.gid),str(tod),str(dow),seed = participant.rando_gen)\n",
    "            elif time.hour in experiment.location_update_hours and time.minute==0 :\n",
    "                location = feat_trans.get_next_location(participant.gid,tod,dow,participant.get_loc(),seed =participant.rando_gen)\n",
    "\n",
    "            participant.set_loc(location)\n",
    "\n",
    "            if time <= participant.times[0]:\n",
    "                        steps_last_time_period = 0\n",
    "            else:\n",
    "                if time.hour==0 and time.minute==0:\n",
    "                    participant.current_day_counter=participant.current_day_counter+1\n",
    "\n",
    "                steps_last_time_period = participant.steps\n",
    "\n",
    "            prob = .6\n",
    "            add=None\n",
    "            optimal_action = -1\n",
    "            optimal_reward = -100\n",
    "            if dt:\n",
    "                if policy=='TS':\n",
    "                    pretreatment = feat_trans.get_pretreatment(steps_last_time_period)\n",
    "                    z = [1]\n",
    "\n",
    "                    if 'tod' in global_policy_params.baseline_features:\n",
    "                        z.append(tod)\n",
    "                    if 'dow' in global_policy_params.baseline_features:\n",
    "                        z.append(dow)\n",
    "                    if 'pretreatment' in global_policy_params.baseline_features:\n",
    "                        z.append(pretreatment)\n",
    "                    if 'location' in global_policy_params.baseline_features:\n",
    "                        z.append(location)\n",
    "\n",
    "                    if algo_type=='batch':\n",
    "                        prob = TS.prob_cal_ts(z,0,global_policy_params.mus2,global_policy_params.sigmas2,global_policy_params,seed = experiment.algo_rando_gen)\n",
    "                    elif algo_type=='personalized':\n",
    "                        prob = TS.prob_cal_ts(z,0,personal_policy_params.mus2[participant.pid],personal_policy_params.sigmas2[participant.pid],global_policy_params,seed=experiment.algo_rando_gen)\n",
    "                    elif algo_type=='pooling':\n",
    "                        prob = TS.prob_cal_ts(z,0,personal_policy_params.mus2[participant.pid],personal_policy_params.sigmas2[participant.pid],global_policy_params,seed=experiment.algo_rando_gen)\n",
    "                 \n",
    "                    action = int(experiment.algo_rando_gen.uniform() < prob)\n",
    "                    if availability:\n",
    "                        context = [action,participant.gid,tod,dow,weather,pretreatment,location,\\\n",
    "                                       0,0,0]\n",
    "                        steps = feat_trans.get_steps_action(context,seed = participant.rando_gen)\n",
    "                        add = action*(feat_trans.get_add_no_action(z,experiment.beta,participant.Z))\n",
    "                        participant.steps = steps+add\n",
    "                        optimal_reward = get_optimal_reward(experiment.beta,z,participant.Z)\n",
    "                        optimal_action = int(optimal_reward>=0)\n",
    "                    else:\n",
    "                        steps = feat_trans.get_steps_no_action(participant.gid,tod,dow,location,\\\n",
    "                        pretreatment,weather,seed = participant.rando_gen)\n",
    "                        participant.steps = steps\n",
    "\n",
    "                    global_policy_params.decision_times =   global_policy_params.decision_times+1\n",
    "                else:\n",
    "                        steps = feat_trans.get_steps_no_action(participant.gid,tod,dow,location,\\\n",
    "                                                               pretreatment,weather,seed = participant.rando_gen)\n",
    "                        participant.steps = steps\n",
    "                context_dict =  {'steps':participant.steps,'add':add,'action':action,'location':location,'location_1':int(location==1),\\\n",
    "'ltps':steps_last_time_period,'location_2':int(location==2),'location_3':int(location==3),\\\n",
    "    'study_day':participant.current_day_counter,\\\n",
    "        'decision_time':dt,\\\n",
    "            'time':time,'avail':availability,'prob':prob,\\\n",
    "                'dow':dow,'tod':tod,'weather':weather,\\\n",
    "                    'pretreatment':feat_trans.get_pretreatment(steps_last_time_period),\\\n",
    "                        'optimal_reward':optimal_reward,'optimal_action':optimal_action,\\\n",
    "                            'mu2':global_policy_params.mus2,'gid':participant.gid}\n",
    "\n",
    "                participant.history[time]=context_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regret(experiment):\n",
    "    optimal_actions ={}\n",
    "    rewards = {}\n",
    "    actions = {}\n",
    "    for pid,person in experiment.population.items():\n",
    "        for time,data in person.history.items():\n",
    "            if data['decision_time'] and data['avail']:\n",
    "                key = time\n",
    "                if key not in optimal_actions:\n",
    "                    optimal_actions[key]=[]\n",
    "                if key not in rewards:\n",
    "                    rewards[key]=[]\n",
    "                if key not in actions:\n",
    "                    actions[key]=[]\n",
    "                if data['optimal_action']!=-1:\n",
    "                    optimal_actions[key].append(int(data['action']==data['optimal_action']))\n",
    "                    regret = int(data['action']!=data['optimal_action'])*(abs(data['optimal_reward']))\n",
    "                    rewards[key].append(regret)\n",
    "                    actions[key].append(data['action'])\n",
    "    return optimal_actions,rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_groupids(exp):\n",
    "    to_save  = {}\n",
    "    for pid,pdata in exp.population.items():\n",
    "        gid  = pdata.gid\n",
    "        key = 'participant-{}'.format(pid)\n",
    "        to_save[key]=gid\n",
    "    return to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_many(algo_type,cases,sim_start,sim_end,update_time,dist_root,write_directory):\n",
    "    for case in cases:\n",
    "        #,'case_two','case_three'\n",
    "        #case = 'case_one'\n",
    "        \n",
    "        \n",
    "        baseline = ['tod','dow','pretreatment','location']\n",
    "        \n",
    "        \n",
    "        \n",
    "        for u in [update_time]:\n",
    "            \n",
    "            all_actions = {}\n",
    "            all_rewards = {}\n",
    "            #'../../Downloads/distributions/'\n",
    "            feat_trans = ft.feature_transformation(dist_root)\n",
    "            \n",
    "            for sim in range(sim_start,sim_end):\n",
    "                pop_size=32\n",
    "                experiment = study.study(dist_root,pop_size,'_short_unstaggered_6',which_gen=case,sim_number=sim)\n",
    "                experiment.update_beta(set(baseline))\n",
    "                #print('beta')\n",
    "                #print(experiment.beta)\n",
    "                glob,personal = initialize_policy_params_TS(experiment,7,standardize=False,baseline_features=baseline,psi_features=['location'],responsivity_keys=baseline,algo_type =algo_type)\n",
    "                print(glob.noise_term)\n",
    "                print(glob.sigma_u)\n",
    "                #glob.noise_term  =1.3\n",
    "                hist = new_kind_of_simulation(experiment,'TS',personal,glob,feat_trans=feat_trans,algo_type=algo_type,case=case,sim_num=sim)\n",
    "                to_save = make_to_save(experiment)\n",
    "                actions,rewards = get_regret(experiment)\n",
    "                gids = make_to_groupids(experiment)\n",
    "                    #for i,a in actions.items():\n",
    "                    #if i not in all_actions:\n",
    "                    #all_actions[i]=a\n",
    "                    #else:\n",
    "                    #all_actions[i].extend(a)\n",
    "                    #for i,a in rewards.items():\n",
    "                    # if i not in all_rewards:\n",
    "                    #all_rewards[i]=a\n",
    "                    #else:\n",
    "                    #all_rewards[i].extend(a)\n",
    "            \n",
    "                #return experiment,personal\n",
    "                filename = '{}{}/population_size_{}_update_days_{}_{}_static_sim_{}_alltests.pkl'.format('{}{}/'.format(write_directory,algo_type),case,pop_size,u,'short',sim)\n",
    "                with open(filename,'wb') as f:\n",
    "                    pickle.dump({'gids':gids,'regrets':rewards,'actions':actions,'history':to_save},f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.304\n",
      "[[ 0.18   -0.1977]\n",
      " [-0.1977  0.4234]]\n",
      "steps 1.3210221737273073\n",
      "{'sigma_u': array([[ 0.03088497, -0.07971465],\n",
      "       [-0.07971465,  0.27416387]]), 'cov': array([[1.530885 , 1.530885 , 1.530885 , ..., 1.       , 1.       ,\n",
      "        1.       ],\n",
      "       [1.530885 , 1.530885 , 1.530885 , ..., 1.       , 1.       ,\n",
      "        1.       ],\n",
      "       [1.530885 , 1.530885 , 3.030885 , ..., 1.       , 2.       ,\n",
      "        2.       ],\n",
      "       ...,\n",
      "       [1.       , 1.       , 1.       , ..., 3.030885 , 3.030885 ,\n",
      "        3.030885 ],\n",
      "       [1.       , 1.       , 2.       , ..., 3.030885 , 6.0308847,\n",
      "        6.0308847],\n",
      "       [1.       , 1.       , 2.       , ..., 3.030885 , 6.0308847,\n",
      "        6.0308847]], dtype=float32), 'noise': 1.41227388381958, 'like': 0}\n",
      "steps 1.3360981007899224\n",
      "{'sigma_u': array([[ 0.04165911, -0.10402793],\n",
      "       [-0.10402793,  0.27388665]]), 'cov': array([[1.5416591, 1.5416591, 1.5416591, ..., 1.       , 1.5      ,\n",
      "        1.       ],\n",
      "       [1.5416591, 1.5416591, 1.5416591, ..., 1.       , 1.5      ,\n",
      "        1.       ],\n",
      "       [1.5416591, 1.5416591, 3.041659 , ..., 2.       , 3.       ,\n",
      "        2.       ],\n",
      "       ...,\n",
      "       [1.       , 1.       , 2.       , ..., 6.7861805, 5.496251 ,\n",
      "        6.7861805],\n",
      "       [1.5      , 1.5      , 3.       , ..., 5.496251 , 8.507489 ,\n",
      "        5.496251 ],\n",
      "       [1.       , 1.       , 2.       , ..., 6.7861805, 5.496251 ,\n",
      "        6.7861805]], dtype=float32), 'noise': 1.4140751361846924, 'like': 0}\n",
      "steps 1.3480628588944465\n",
      "{'sigma_u': array([[ 0.03295215, -0.02532297],\n",
      "       [-0.02532297,  0.02622001]]), 'cov': array([[1.5329522, 1.5329522, 1.5329522, ..., 1.5      , 1.5      ,\n",
      "        1.       ],\n",
      "       [1.5329522, 1.5329522, 1.5329522, ..., 1.5      , 1.5      ,\n",
      "        1.       ],\n",
      "       [1.5329522, 1.5329522, 3.032952 , ..., 3.       , 1.5      ,\n",
      "        2.       ],\n",
      "       ...,\n",
      "       [1.5      , 1.5      , 3.       , ..., 8.408525 , 6.7204227,\n",
      "        5.330046 ],\n",
      "       [1.5      , 1.5      , 1.5      , ..., 6.7204227, 6.71241  ,\n",
      "        3.9885108],\n",
      "       [1.       , 1.       , 2.       , ..., 5.330046 , 3.9885108,\n",
      "        6.4341025]], dtype=float32), 'noise': 1.4880553483963013, 'like': 0}\n",
      "steps 1.3500107557022614\n",
      "{'sigma_u': array([[ 0.030729  , -0.08999011],\n",
      "       [-0.08999011,  0.27381426]]), 'cov': array([[1.530729, 1.530729, 1.530729, ..., 1.5     , 1.5     , 1.5     ],\n",
      "       [1.530729, 1.530729, 1.530729, ..., 1.5     , 1.5     , 1.5     ],\n",
      "       [1.530729, 1.530729, 3.030729, ..., 3.      , 3.      , 3.      ],\n",
      "       ...,\n",
      "       [1.5     , 1.5     , 3.      , ..., 6.215067, 4.619148, 4.619148],\n",
      "       [1.5     , 1.5     , 3.      , ..., 4.619148, 4.999655, 4.999655],\n",
      "       [1.5     , 1.5     , 3.      , ..., 4.619148, 4.999655, 4.999655]],\n",
      "      dtype=float32), 'noise': 1.4134782552719116, 'like': 0}\n",
      "steps 1.3452540444671195\n",
      "{'sigma_u': array([[ 0.03518791, -0.01686546],\n",
      "       [-0.01686546,  0.01316043]]), 'cov': array([[1.535188 , 1.535188 , 1.535188 , ..., 1.5      , 1.5      ,\n",
      "        1.5      ],\n",
      "       [1.535188 , 1.535188 , 1.535188 , ..., 1.5      , 1.5      ,\n",
      "        1.5      ],\n",
      "       [1.535188 , 1.535188 , 3.035188 , ..., 1.5      , 3.       ,\n",
      "        3.       ],\n",
      "       ...,\n",
      "       [1.5      , 1.5      , 1.5      , ..., 5.0751877, 3.1888852,\n",
      "        3.1888852],\n",
      "       [1.5      , 1.5      , 3.       , ..., 3.1888852, 6.1762376,\n",
      "        6.1762376],\n",
      "       [1.5      , 1.5      , 3.       , ..., 3.1888852, 6.1762376,\n",
      "        6.1762376]], dtype=float32), 'noise': 1.4310494661331177, 'like': 0}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Downloads/pooling_results/pooling/case_two/population_size_32_update_days_7_short_static_sim_0_alltests.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-15cd3239bab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pooling'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'case_two'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../../Downloads/distributions/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../../Downloads/pooling_results/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c0c482c2981d>\u001b[0m in \u001b[0;36mrun_many\u001b[0;34m(algo_type, cases, sim_start, sim_end, update_time, dist_root, write_directory)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m#return experiment,personal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}{}/population_size_{}_update_days_{}_{}_static_sim_{}_alltests.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}{}/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malgo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpop_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'short'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'gids'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'regrets'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'history'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mto_save\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Downloads/pooling_results/pooling/case_two/population_size_32_update_days_7_short_static_sim_0_alltests.pkl'"
     ]
    }
   ],
   "source": [
    "e,p = run_many('pooling',['case_two'],0,1,7,'../../Downloads/distributions/','../../Downloads/pooling_results/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isreal(np.array([[-4.47531231e-03+0.j        , -2.61702768e-18-0.04273931j],\n",
    "       [-2.61702768e-18-0.04273931j,  6.91121340e-01+0.j        ]])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 1.3210221737273073\n",
      "Iter 1/10 - Loss: 1.643\n",
      "Iter 2/10 - Loss: 1.650\n",
      "Iter 3/10 - Loss: 1.649\n",
      "Iter 4/10 - Loss: 1.643\n",
      "Iter 5/10 - Loss: 1.636\n",
      "Iter 6/10 - Loss: 1.636\n",
      "Iter 7/10 - Loss: 1.631\n",
      "Iter 8/10 - Loss: 1.627\n",
      "Iter 9/10 - Loss: 1.624\n",
      "not enough values to unpack (expected 3, got 2)\n",
      "here\n",
      "{'sigma_u': array([[ 0.02072786, -0.05170246],\n",
      "       [-0.05170246,  0.14851183]]), 'cov': array([[1.5207279, 1.5207279, 1.5207279, ..., 1.       , 1.       ,\n",
      "        1.       ],\n",
      "       [1.5207279, 1.5207279, 1.5207279, ..., 1.       , 1.       ,\n",
      "        1.       ],\n",
      "       [1.5207279, 1.5207279, 3.0207279, ..., 1.       , 2.       ,\n",
      "        2.       ],\n",
      "       ...,\n",
      "       [1.       , 1.       , 1.       , ..., 3.0207279, 3.0207279,\n",
      "        3.0207279],\n",
      "       [1.       , 1.       , 2.       , ..., 3.0207279, 6.0207276,\n",
      "        6.0207276],\n",
      "       [1.       , 1.       , 2.       , ..., 3.0207279, 6.0207276,\n",
      "        6.0207276]], dtype=float32), 'noise': 1.4619027376174927, 'like': 0}\n",
      "steps 1.336886180263299\n",
      "Iter 1/10 - Loss: 1.630\n",
      "Iter 2/10 - Loss: 1.626\n",
      "Iter 3/10 - Loss: 1.625\n",
      "Iter 4/10 - Loss: 1.624\n",
      "Iter 5/10 - Loss: 1.624\n",
      "Iter 6/10 - Loss: 1.621\n",
      "Iter 7/10 - Loss: 1.620\n",
      "Iter 8/10 - Loss: 1.620\n",
      "Iter 9/10 - Loss: 1.618\n",
      "not enough values to unpack (expected 3, got 2)\n",
      "here\n",
      "{'sigma_u': array([[ 0.03915728, -0.06980051],\n",
      "       [-0.06980051,  0.14215498]]), 'cov': array([[1.5391573, 1.5391573, 1.5391573, ..., 1.5      , 1.5      ,\n",
      "        1.       ],\n",
      "       [1.5391573, 1.5391573, 1.5391573, ..., 1.5      , 1.5      ,\n",
      "        1.       ],\n",
      "       [1.5391573, 1.5391573, 3.0391574, ..., 3.       , 3.       ,\n",
      "        2.       ],\n",
      "       ...,\n",
      "       [1.5      , 1.5      , 3.       , ..., 6.1056786, 6.2563186,\n",
      "        4.4633574],\n",
      "       [1.5      , 1.5      , 3.       , ..., 6.2563186, 8.44171  ,\n",
      "        5.456319 ],\n",
      "       [1.       , 1.       , 2.       , ..., 4.4633574, 5.456319 ,\n",
      "        6.8210363]], dtype=float32), 'noise': 1.4899749755859375, 'like': 0}\n",
      "steps 1.3495963969854685\n",
      "Iter 1/10 - Loss: 1.626\n",
      "Iter 2/10 - Loss: 1.623\n",
      "Iter 3/10 - Loss: 1.629\n",
      "Iter 4/10 - Loss: 1.622\n",
      "Iter 5/10 - Loss: 1.621\n",
      "Iter 6/10 - Loss: 1.621\n",
      "Iter 7/10 - Loss: 1.622\n",
      "Iter 8/10 - Loss: 1.620\n",
      "Iter 9/10 - Loss: 1.618\n",
      "not enough values to unpack (expected 3, got 2)\n",
      "here\n",
      "{'sigma_u': array([[ 0.03353339, -0.08075571],\n",
      "       [-0.08075571,  0.24154897]]), 'cov': array([[1.5335333, 1.5335333, 1.5335333, ..., 1.5      , 1.5      ,\n",
      "        1.5      ],\n",
      "       [1.5335333, 1.5335333, 1.5335333, ..., 1.5      , 1.5      ,\n",
      "        1.5      ],\n",
      "       [1.5335333, 1.5335333, 3.0335333, ..., 3.       , 1.5      ,\n",
      "        3.       ],\n",
      "       ...,\n",
      "       [1.5      , 1.5      , 3.       , ..., 8.513571 , 6.8014965,\n",
      "        6.4706144],\n",
      "       [1.5      , 1.5      , 1.5      , ..., 6.8014965, 6.770851 ,\n",
      "        4.869425 ],\n",
      "       [1.5      , 1.5      , 3.       , ..., 6.4706144, 4.869425 ,\n",
      "        6.2906265]], dtype=float32), 'noise': 1.4950439929962158, 'like': 0}\n",
      "steps 1.3510720270948147\n",
      "Iter 1/10 - Loss: 1.626\n",
      "Iter 2/10 - Loss: 1.625\n",
      "Iter 3/10 - Loss: 1.622\n",
      "Iter 4/10 - Loss: 1.620\n",
      "Iter 5/10 - Loss: 1.619\n",
      "Iter 6/10 - Loss: 1.618\n",
      "Iter 7/10 - Loss: 1.618\n",
      "Iter 8/10 - Loss: 1.617\n",
      "Iter 9/10 - Loss: 1.617\n",
      "Iter 10/10 - Loss: 1.615\n",
      "{'sigma_u': array([[ 0.01300428, -0.02216649],\n",
      "       [-0.02216649,  0.04229445]]), 'cov': array([[1.5130043, 1.5130043, 1.5130043, ..., 1.5      , 1.5      ,\n",
      "        1.5      ],\n",
      "       [1.5130043, 1.5130043, 1.5130043, ..., 1.5      , 1.5      ,\n",
      "        1.5      ],\n",
      "       [1.5130043, 1.5130043, 3.0130043, ..., 3.       , 3.       ,\n",
      "        3.       ],\n",
      "       ...,\n",
      "       [1.5      , 1.5      , 3.       , ..., 6.0110497, 4.485071 ,\n",
      "        4.485071 ],\n",
      "       [1.5      , 1.5      , 3.       , ..., 4.485071 , 5.0414815,\n",
      "        5.0414815],\n",
      "       [1.5      , 1.5      , 3.       , ..., 4.485071 , 5.0414815,\n",
      "        5.0414815]], dtype=float32), 'noise': 1.4799747467041016, 'like': 0}\n",
      "steps 1.3458996214336252\n",
      "Iter 1/10 - Loss: 1.613\n",
      "Iter 2/10 - Loss: 1.613\n",
      "Iter 3/10 - Loss: 12.377\n",
      "Iter 4/10 - Loss: 1.611\n",
      "Iter 5/10 - Loss: 1.610\n",
      "Iter 6/10 - Loss: 1.611\n",
      "Iter 7/10 - Loss: 1.612\n",
      "Iter 8/10 - Loss: 1.612\n",
      "Iter 9/10 - Loss: 1.612\n",
      "Iter 10/10 - Loss: 1.612\n",
      "{'sigma_u': array([[ 0.16904131, -0.21181661],\n",
      "       [-0.21181661,  0.34134901]]), 'cov': array([[1.6690413, 1.6690413, 1.6690413, ..., 1.5      , 1.       ,\n",
      "        1.5      ],\n",
      "       [1.6690413, 1.6690413, 1.6690413, ..., 1.5      , 1.       ,\n",
      "        1.5      ],\n",
      "       [1.6690413, 1.6690413, 3.1690414, ..., 1.5      , 2.       ,\n",
      "        3.       ],\n",
      "       ...,\n",
      "       [1.5      , 1.5      , 1.5      , ..., 5.209041 , 2.6690047,\n",
      "        3.0690048],\n",
      "       [1.       , 1.       , 2.       , ..., 2.6690047, 6.9013734,\n",
      "        4.528773 ],\n",
      "       [1.5      , 1.5      , 3.       , ..., 3.0690048, 4.528773 ,\n",
      "        6.1561728]], dtype=float32), 'noise': 1.5153429508209229, 'like': 0}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Downloads/pooling_results/pooling/case_two/population_size_32_update_days_7_short_static_sim_0_alltests.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-15cd3239bab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pooling'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'case_two'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../../Downloads/distributions/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../../Downloads/pooling_results/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-4bee447828e5>\u001b[0m in \u001b[0;36mrun_many\u001b[0;34m(algo_type, cases, sim_start, sim_end, update_time, dist_root, write_directory)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m#return experiment,personal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}{}/population_size_{}_update_days_{}_{}_static_sim_{}_alltests.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}{}/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malgo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpop_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'short'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'gids'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'regrets'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'actions'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'history'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mto_save\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Downloads/pooling_results/pooling/case_two/population_size_32_update_days_7_short_static_sim_0_alltests.pkl'"
     ]
    }
   ],
   "source": [
    "e,p = run_many('pooling',['case_three'],0,1,7,'../../Downloads/distributions/','../../Downloads/pooling_results/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tone = torch.tensor([[2,3,4,5,6,1],[2,3,3,5,6,1]])\n",
    "ttwo = torch.tensor([[0,0,0,0,0,3],[1,0,0,0,0,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((tone[:,0],tone[:,1]),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = torch.stack([tone[:,i] for  i in [0,1]],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.1\n"
     ]
    }
   ],
   "source": [
    "import gpytorch\n",
    "print(gpytorch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
