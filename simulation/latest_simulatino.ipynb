{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append('../models')\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import sim_functions_cleaner  as sf\n",
    "import operator\n",
    "import study\n",
    "import time\n",
    "import TS_personal_params_pooled as pp\n",
    "import TS_global_params_pooled as gtp\n",
    "from numpy.random import uniform\n",
    "\n",
    "#sys.path.append('../simulation')\n",
    "import TS_fancy_pooled\n",
    "#import TS_fancy_pooled\n",
    "import eta\n",
    "import pooling_bandits as pb\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_policy_params_TS(experiment):\n",
    "    \n",
    "    global_p =gtp.TS_global_params(10)\n",
    "    personal_p = pp.TS_personal_params()\n",
    "    #global_p =gtp.TS_global_params(10,context_dimension)\n",
    "    \n",
    "    global_p.kdim = 11\n",
    "    global_p.baseline_indices = [0,1,2,3,4,5,6]\n",
    "    global_p.psi_indices = [4,6]\n",
    "    global_p.user_id_index = 7\n",
    "    global_p.user_day_index =8\n",
    "    \n",
    "    global_p.baseline_features = ['location','weather']\n",
    "    global_p.psi_features = ['location']\n",
    "    \n",
    "    #print(type(personal_p))\n",
    "    \n",
    "    for person in experiment.population.keys():\n",
    "        experiment.population[person].root = '../../Downloads/distributions/'\n",
    "        initial_context = [0 for i in range(global_p.theta_dim)]\n",
    "        personal_p.mus0[person]= global_p.get_mu0(initial_context)\n",
    "        personal_p.mus1[person]= global_p.get_mu1(initial_context)\n",
    "        personal_p.mus2[person]= global_p.get_mu2(initial_context)\n",
    "        \n",
    "        personal_p.sigmas0[person]= global_p.get_asigma(len( personal_p.mus0[person]))\n",
    "        personal_p.sigmas1[person]= global_p.get_asigma(len( personal_p.mus1[person]))\n",
    "        personal_p.sigmas2[person]= global_p.get_asigma(len( personal_p.mus2[person]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        personal_p.batch[person]=[[] for i in range(len(experiment.person_to_time[person]))]\n",
    "        personal_p.batch_index[person]=0\n",
    "        \n",
    "        personal_p.etas[person]=eta.eta()\n",
    "        \n",
    "        personal_p.last_update[person]=experiment.person_to_time[person][0]\n",
    "    \n",
    "    global_p.write_directory = '../temp'\n",
    "    return global_p ,personal_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_history(write_dir,dt):\n",
    "    to_return = {}\n",
    "    for d in [f for f in os.listdir(write_dir) if f!='.DS_Store']:\n",
    "        participant = {}\n",
    "        for f in os.listdir('{}/{}'.format(write_dir,d)):\n",
    "            if f!='.DS_Store':\n",
    "                time = int(f.split('_')[1])\n",
    "                if time <=dt:\n",
    "                    with open('{}/{}/{}'.format(write_dir,d,f),'rb') as f:\n",
    "                        ld = pickle.load(f)\n",
    "                    participant[time]=ld\n",
    "                    \n",
    "        pid = d.split('_')[1]\n",
    "        if len(participant)>0:\n",
    "            to_return[int(pid)]=participant\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redo_simulation(experiment,policy=None,personal_policy_params=None,global_policy_params=None):\n",
    "    \n",
    "    for time in experiment.study_days:\n",
    "        if time==experiment.last_update_day+pd.DateOffset(days=1):\n",
    "            experiment.last_update_day=time\n",
    "            ##reset global parameters\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_kind_of_simulation(experiment,policy=None,personal_policy_params=None,global_policy_params=None):\n",
    "    #write_directory = '../../murphy_lab/lab/pooling/temp'\n",
    "\n",
    "    for time in experiment.study_days:\n",
    "           \n",
    "            #if time> experiment.study_days[0]:\n",
    "                #history  = pb.make_history(experiment)\n",
    "            if time==experiment.last_update_day+pd.DateOffset(days=1):\n",
    "                experiment.last_update_day=time\n",
    "                ##global update\n",
    "                #print(time)\n",
    "                #print(experiment.last_update_day+pd.DateOffset(days=1))\n",
    "                ##am i checking the current time (need to check the \n",
    "                #current time make sure i'm not using all of the history)\n",
    "               \n",
    "                #print(history)\n",
    "                \n",
    "                ##these lines\n",
    "\n",
    "                #print(history[0].shape)\n",
    "                    #make_history_new(write_directory,.6,global_policy_params)\n",
    "               #print(temp_params['cov'].shape)\n",
    "                #print(temp_params)\n",
    "                #print(temp_params['cov'].shape)\n",
    "                #print(type(temp_params['cov']))\n",
    "                \n",
    "                #del temp_params\n",
    "                ##update global params using these temp_params\n",
    "                \n",
    "                \n",
    "                \n",
    "            ##update global context\n",
    "            ##global context shared across all participants\n",
    "            tod = sf.get_time_of_day(time)\n",
    "            dow = sf.get_day_of_week(time)\n",
    "            if time==experiment.study_days[0]:\n",
    "                \n",
    "                weather = sf.get_weather_prior(tod,time.month)\n",
    "            elif time.hour in experiment.weather_update_hours and time.minute==0:\n",
    "                weather = sf.get_next_weather(str(tod),str(time.month),weather)\n",
    "            ##location depends on person \n",
    "            \n",
    "            for person in experiment.dates_to_people[time]:\n",
    "                dt=False\n",
    "                action = 0 \n",
    "                prob=0\n",
    "                #1\n",
    "                ##for every active person update person specific aspects of their context\n",
    "                participant = experiment.population[person]\n",
    "                #update global context variables\n",
    "                participant.set_tod(tod)\n",
    "                participant.set_dow(dow)\n",
    "                participant.set_wea(weather)\n",
    "                \n",
    "                \n",
    "                availability = (uniform() < 0.8)\n",
    "                participant.set_available(availability)\n",
    "                \n",
    "                if time == participant.times[0]:\n",
    "                    #get first location \n",
    "                    location = sf.get_location_prior(str(participant.gid),str(dow),str(tod))\n",
    "                    participant.set_inaction_duration(0)\n",
    "                    participant.set_action_duration(0)\n",
    "                    participant.set_duration(0)\n",
    "                    participant.set_dosage(0)\n",
    "                    #personal_policy_params.etas[participant.pid]\n",
    "                    \n",
    "                    \n",
    "                if time <= participant.times[1]:\n",
    "                    steps_last_time_period = 0  \n",
    "                    \n",
    "                    ##set first pre-treatment, yesterday step count, variation and dosage\n",
    "                else:\n",
    "                    \n",
    "                    if time.hour==0 and time.minute==0:\n",
    "                        participant.current_day_counter=participant.current_day_counter+1\n",
    "                    \n",
    "                    #print(time)\n",
    "                    steps_last_time_period = participant.steps\n",
    "                \n",
    "                 \n",
    "                    #get var id\n",
    "                    \n",
    "                #if time.date() <= participant.times[0].date():\n",
    "                    #steps_yesterday = 0    \n",
    "                #else:\n",
    "                    #steps_yesterday =  participant.find_yesterday_steps(time)\n",
    "                    #steps_yesterday = sf.to_yid(steps_yesterday)\n",
    "                steps_yesterday=0    \n",
    "                if time.hour in experiment.location_update_hours:\n",
    "                    location = sf.get_next_location(participant.gid,dow,tod,participant.get_loc())\n",
    "                \n",
    "                if time.date()>(participant.times[0]+pd.DateOffset(days=1)).date():\n",
    "                  \n",
    "                    if time.hour==0 and time.minute==0:\n",
    "                        variation = 0\n",
    "                        #participant.find_variation(time)\n",
    "                else:\n",
    "                    variation = 1 \n",
    "                \n",
    "                participant.set_loc(location)\n",
    "                ##maybe faster to update instead of query?\n",
    "                #participant.set_last_time_period_steps(steps_last_time_period)\n",
    "                #participant.set_yesterday_steps(steps_yesterday)\n",
    "                #participant.set_variation(variation)\n",
    "                \n",
    "                ##continue\n",
    "                #2\n",
    "                ##for every active person take an action according to current context, policy, and parameters\n",
    "                \n",
    "                \n",
    "                ##for now:\n",
    "                ##eval with empty array \n",
    "                if time in participant.decision_times:\n",
    "                                        #print(personal_policy_params.batch_index[participant.pid])\n",
    "                    \n",
    "                    \n",
    "                    ##if we have made no global updates\n",
    "\n",
    "                            \n",
    "                    \n",
    "                        \n",
    "                    \n",
    "                    \n",
    "                   \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    dt=True\n",
    "                    action=0\n",
    "                    \n",
    "                    \n",
    "                    if global_policy_params.decision_times>1:\n",
    "                            if   not global_policy_params.updated_cov:\n",
    "                                 global_policy_params.update_cov(global_policy_params.decision_times)   \n",
    "                            print( global_policy_params.decision_times)\n",
    "                            history = pb.make_history_new(.6,glob)\n",
    "                    ##update my mu2 and sigma2\n",
    "                            temp = pb.calculate_posterior(global_policy_params,\\\n",
    "                                                  participant.pid,participant.current_day_counter,\\\n",
    "                                                  history[0], history[1] )\n",
    "                    else:\n",
    "                        temp = [personal_policy_params.mus2[participant.pid],personal_policy_params.sigmas2[participant.pid]]\n",
    "                    mu_beta = temp[0]\n",
    "                    Sigma_beta = temp[1]\n",
    "                    personal_policy_params.update_mus(participant.pid,mu_beta,2)\n",
    "                    personal_policy_params.update_sigmas(participant.pid,Sigma_beta,2)    \n",
    "                    \n",
    "                    \n",
    "                    if policy==None:\n",
    "                        action = sf.get_action(policy)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    elif policy=='TS':\n",
    "                        #some context slice\n",
    "                            prob = TS.prob_cal_ts([int(tod),int(dow)],participant.dosage,\\\n",
    "                                              personal_policy_params.mus2[participant.pid],personal_policy_params.sigmas2[participant.pid],\\\n",
    "                                                 global_policy_params)\n",
    "                            action = int(uniform() < prob)\n",
    "                            \n",
    "                        \n",
    "                            \n",
    "                    elif policy=='TS_fancy':\n",
    "                        #previous\n",
    "                        #Z, X, mu_beta, Sigma_beta, init,current_eta\n",
    "                        \n",
    "                        ##need to make eta part of the global policy params\n",
    "                        prob = TS_fancy_pooled.prob_cal([location,weather,steps_last_time_period,variation,steps_yesterday],participant.dosage,\\\n",
    "                                              personal_policy_params.mus2[participant.pid],personal_policy_params.sigmas2[participant.pid],\\\n",
    "                                                 global_policy_params,personal_policy_params.etas[participant.pid])\n",
    "                        action = int(uniform() < prob)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                    ##is this the same as in the TS?\n",
    "                    ##don't think so, but for now keep like this\n",
    "                    ##no it isn't, i have to redo this\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    if availability:\n",
    "                    \n",
    "\n",
    "                   \n",
    "                    \n",
    "                    \n",
    "                        participant.update_dosage(action)\n",
    "                    \n",
    "                        context = [action,participant.gid,tod,dow,location,weather,sf.get_pretreatment(participant.steps),\\\n",
    "                              steps_yesterday,variation,sf.dosage_to_dosage_key(participant.dosage)]\n",
    "                    \n",
    "                        participant.steps_last_time_period = participant.steps\n",
    "                        steps = sf.get_steps_action(context)\n",
    "                        participant.steps = steps\n",
    "                    else:\n",
    "                        participant.steps_last_time_period = participant.steps\n",
    "                        steps = sf.get_steps_no_action(participant.gid,tod,dow,location,weather,participant.steps)\n",
    "                        participant.steps = steps\n",
    "                        \n",
    "                \n",
    "                    my_directory = '{}/participant_{}'.format(global_policy_params.write_directory,participant.pid)\n",
    "                    if not os.path.exists(my_directory):\n",
    "                        os.makedirs(my_directory)\n",
    "                    with open('{}/day_{}'.format(my_directory,global_policy_params.decision_times),'wb') as f:\n",
    "                        pickle.dump(context_dict,f)\n",
    "                        \n",
    "                        \n",
    "                    global_policy_params.decision_times =   global_policy_params.decision_times+1\n",
    "                    history =pb.make_history_new(.6,glob)\n",
    "                    temp_params = TS_fancy_pooled.global_updates(history[0],history[1],global_policy_params,train_type = 'empirical_bayes')\n",
    "                    global_policy_params.update_params(temp_params)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                        participant.steps_last_time_period = participant.steps\n",
    "                        steps = sf.get_steps_no_action(participant.gid,tod,dow,location,weather,participant.steps)\n",
    "                        participant.steps = steps     \n",
    "                \n",
    "                ##history:\n",
    "                context_dict =  {'steps':steps,'action':action,'weather':weather,'location':location,\\\n",
    "                                'ltps':steps_last_time_period,'duration':participant.duration,\\\n",
    "                                'study_day':participant.current_day_counter,'decision_time':dt}\n",
    "                #participant.history[time]=context_dict\n",
    "                \n",
    "            #3\n",
    "            ##for every active person generate a step count given current context\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ##update at midnight (here we have ensured that no one has a ) experiment.update_hour\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../temp'\n",
    "for the_file in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, the_file)\n",
    "    try:\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = study.study('../../Downloads/distributions/')\n",
    "glob,personal = initialize_policy_params_TS(experiment)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22183314]]\n",
      "[[ 1.   2.   1.   0.6  1.2 -0.6 -1.2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gpflow.logdensities:Shape of x must be 2D at computation.\n",
      "WARNING:gpflow.logdensities:Shape of mu may be unknown or not 2D.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "called\n",
      "True\n",
      "2\n",
      "[[0.16993575]]\n",
      "[[ 1.   2.   1.   0.6  1.2 -0.6 -1.2]]\n",
      "[[0.28489138 0.        ]\n",
      " [0.         0.03322134]]\n",
      "[[ 1.   2.   1.   0.6  1.2 -0.6 -1.2]\n",
      " [ 1.   2.   1.   0.6  1.2 -0.6 -1.2]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,2) (2,7) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cdf986c40757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_kind_of_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'TS_fancy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpersonal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-bcf28a06c389>\u001b[0m in \u001b[0;36mnew_kind_of_simulation\u001b[0;34m(experiment, policy, personal_policy_params, global_policy_params)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0mglobal_policy_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_times\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mglobal_policy_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_times\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_history_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                     \u001b[0mtemp_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTS_fancy_pooled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_policy_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'empirical_bayes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0mglobal_policy_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pooling_rl/models/pooling_bandits.py\u001b[0m in \u001b[0;36mmake_history_new\u001b[0;34m(pi, glob)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_standardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;31m#new_x = preprocessing.scale(np.array(ad[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m#new_y = preprocessing.scale(np.array(ad[1]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pooling_rl/models/pooling_bandits.py\u001b[0m in \u001b[0;36mnew_standardize\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mnew_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0mmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mnew_x\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,2) (2,7) "
     ]
    }
   ],
   "source": [
    "new_kind_of_simulation(experiment,'TS_fancy',personal,glob) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.theta_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56197268, 0.80930025, 0.51885868, 0.12019092, 0.75819096,\n",
       "        0.18061007, 0.40971441]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1,7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
